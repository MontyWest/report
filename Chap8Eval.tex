%___________________________________________
%*************************************************************
% Evaluation
%___________________________________________
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

\section{Project Evaluation}
\label{sec:eval}

With the results presented above, we can now critically consider the project in terms of the requirements set out in the project's aim, objectives (Section \ref{subsec:aimobj}) and specification (Section \ref{sec:projspec}).

%-----------------------------------------------------------------------
% Agent Framework
%-----------------------------------------------------------------------

\subsection{Agent Framework}

The primary objective of the agent framework was to enable and encourage meaningful improvement during learning. The focus on a rule based system allowed for agents to be represented with a simple encoding, which afforded a great deal of freedom to the learning module. It reduced development time as several features of the ECJ library could be used as is and any extensions did not have to process a complex data structure. Moreover, it allowed for greater control, which was crucial in honing and revising the learning parameters. Ultimately, we can see from the results that, given the right parameters, the agent framework did enable significant improvement over generation, with later agents achieving near maximum fitness.

However, the results show a high fitness deviation over each generation's population. This is likely due to the inherent sensitivity to mutation of a rule-based approach. One gene mutation can transform a once useful rule into one that is counter productive, with a great affect to the agent's capability.

In addition to the project's aims and objective, three criteria were laid out for the agent's use of a rule based system in Section \ref{sec:projspec}: speed, capability and complexity. The framework design relies heavily on how the agent senses the environment; as such the selection of perceptions is a major factor in its evaluation.

The \textbf{complex} handcrafted ruleset shows that the framework's choice of perceptions allows for capable agents. It rarely fails to finish the easier levels and can successfully tackle the more difficult ones. Furthermore, the depth of the perceptions and the available actions allows for competent behaviour to be demonstrated by relatively small rulesets, which improves the agent's speed. In fact, agent response time never rises above a millisecond. This allows for levels to be run at practically any number of frames per second and has little effect on total learning time.

On the other hand, the framework is still limited. There is no way for any ruleset agent to break itself out of a loop or navigate a dead end. Similarly, it can never develop an effective enemy killing behaviour without differentiate between different types.

However, as discussed in Section \ref{subsec:agentdes}, improving capability by embellishing the agent's perceptions comes at the penalty of complexity. A more complex design lends to a larger search space for the learning algorithm, which hinders the development of a successful agent through random mutation. This was also a crucial consideration of the REALM team, whose V2 agent was designed, in part, to reduce the search space of the VI agent \cite{realm}[p.~86].

In its current form, the agent framework balances these factors reasonably well. Our results show that a successful and interesting agent can be evolved despite the total number of possible rules
\[ \centering
4^4 * 3^7 * 2^4 = 8,957,952
\]
being higher than that of REALM's V1 agent ($7,558,272$) \cite{realm}[p.~86]. Thus, improvements to the capability of the agent should not increase the search space, but instead focus on making each perception is purposeful.

In the LEMMEL run parameters (and in the handcrafted agents) we encouraged the disregarding of the MarioMode and EnemyLeft perceptions. Our learning parameters allowed us to reduce the search space by increasing the probability of the {\footnotesize DONT\_CARE} condition. This is clearly a counter intuitive approach and the agent design should be amended by removing these perceptions. 

Such removals would allow for additional perceptions and/or design extensions without increasing agent complexity. Given more time, a redesign that allowed agent encoding control over its perceptions would be considered. Perceptions would remain fixed, but the parameters that control them would be variable. For example, the area in which an agent looks for an obstacle or enemy could be controlled by parameters on a ruleset wide basis. Moreover, perceptions that ascertain the last used action (a simplistic form of memory) and greater detail in the identifying enemies would also be investigated.


%-----------------------------------------------------------------------
% Level playing
%-----------------------------------------------------------------------

\subsection{Level Playing}

The primary motivation for the level playing module was to meet Objective \ref{obj:learn}: Provide a large and diverse test bed from which to learn. This was reiterated in the project specification (Section \ref{sec:projspec}), which decrees that it must, in a parametrised manner, produce a variety of levels and ways to score an agent.

The former was achieved by providing access to the benchmark's level generation options through simple data classes. Additionally, as detailed in Section \ref{subsec:enginemod}, modifications were made to the LevelGeneration class to provide greater diversity and to ensure parameters were being treated correctly. This was an unexpected workload, and as such could not be afforded ample attention. Without time constraints, further modifications would have been, including improved scaling of enemy types and better block placement. Furthermore, these and previous changes, as well as other level generation features, would have been made parametrisable, allowing the MWLevelOptions class more power and precision.

The latter was achieved by the MWEvaluationMultipliers class, which parametrised a linear function over level playing statistics. The options mirrored the statistics produced by the benchmark software. Whilst this was effective in calculating agent fitness in the learning module, additional statistics would be beneficial. For example, it is currently impossible to punish an agent for the manner in which it failed (i.e. by falling into a pit, by colliding with an enemy, or by running out of time). If the project was to continue, the reliance of the benchmark software for these statistics would be reduced and a specialised EvaluationInfo class would be implemented, allowing for the level playing statistics to be extended.

Specifying and persisting these two elements was accomplished by extending ECJ's parameter file system. Adopting this existing system saved time and ensured it integrated well with the learning module. With the high number of revisions and the emergent importance of the evaluation task during the project, this decision proved vital. Whilst it provided great control when using a small number of levels, it was too verbose to be used for larger level tasks. For example, the comparator task (Section \ref{subsec:comptask} and Appendix \ref{app:comptask}) had to be hard-coded rather than specified in a level options file. With more time, the vocabulary of the parameter persistence would be extended to include controls for segments of episodes and values as a function of episode number (as seen in the comparator task).

We saw in the results that the learning runs featured a high level of inter-generational variance. This is to be expected, especially of the average fitness, due partly to the agent framework's sensitivity to mutation and partly to the nature of Evolutionary Strategies. However, the variance is also displayed by the handcrafted agent, which suggests that variance may in fact be caused by the difficulty of the evaluation task fluctuating between generations. As the only task parameter that changes between generations, the choice of level seed may be having too much of an effect on level generation. Ways to tackle this issue within the parameters of the learning algorithm are discussed in the next subsection, however, if given additional time the influence of the level seed on level difficulty would be investigated.

Aside from level generation, the benchmark software also raised several other barriers to the project. Firstly, there was little to no documentation, which slowed the initial progress of the project. Secondly, it greatly hindered the testability of the project as a whole (as discussed in Section \ref{subsec:lptest}), making it very difficult to provide significant test coverage (as prescribed by Objective \ref{obj:test}). Lastly, it impeded the project's ability to meet the non-functional requirement on thread safety. As mentioned in Section \ref{subsec:learnrunning}, the game-engine is implemented as a singleton, relying heavily on static assets. A continued project would look to refactor this and allow the evaluation stage of the learning process to be multithreaded.



%-----------------------------------------------------------------------
% Learning Process
%-----------------------------------------------------------------------

\subsection{Learning Process}
\label{subsec:evallearn}

Section \ref{subsec:learnparam} described the revision of the learning parameters during the project. The results show that honing these parameters was crucial to the success of the project. Due to time constraints, parameter adjustment had to be stopped to allow for results to be gathered and analysed. Without such a restriction more parameter revisions would have been made.

Firstly, a continued the attempt to reduce generational variance (in conjunction with solutions mentioned in the previous two sections) would be made. We saw that having a single level seed for the entire run removed variance from the best fitness (as expected) and lowered variance in the average fitness. However, it produced a less capable agent, which was overly suited to the small number of levels it played (known as overfitting). Instead a compromise would be struck, perhaps taking seeds from a fixed pool, or have them run in generational segments. In this way agents are evaluated over a wider range of levels, but not at a hindrance to the learning evolutionary process.

Secondly reducing overfitting in general would be investigated. We saw in the results that the learnt agent performed better than the complex handcrafted agent at the learning evaluation task, but did not perform as well on the comparator task. We also saw that in both the fixed and variable seed run that the learnt agent achieved near perfect score relatively early, suggesting the task could be made more difficult. In future revisions the learning task would be run on higher difficulties, with more focus on enemies. Also, with the ability to multithread the evaluation, a more extensive task could be run, without a time penalty. Increase the number of levels would allow the task to be more representative of the comparator task.

\vspace{\baselineskip}

Even without the above revisions we have seen that the use of an Evolutionary Strategy has proved successful. It has allowed for our aim to be achievement, as well as meeting the project specification. Meaningful learning was observed and the final agent demonstrated interesting behaviours, including some that were unforeseen and not considered during the creation of handcrafted rulesets. Moreover, as the use of truncation selection is known to reduce variance in learning algorithms \cite[s.~3.8.3]{geatbx} and due to the variance induced by both the agent framework's mutation sensitivity and the level playing's seed variation, the choice of an evolution strategy turned out to be very appropriate.

However, because of the scope of the project, its implementation is quite simplistic. With more time several additional features would be considered. First would be the use of rule crossover, as seen in the REALM team's approach (described in Section \ref{ssec:realm}). Second would be utilising a Tabu (taboo) list, a common approach in optimisation search algorithms \cite{tabu}. Rules that caused a significant drop in fitness would be added to a blacklist and would be avoided by future mutations. Last would be adaptive mutation rates, a common feature of ESes \cite[s.~4]{es-book}. The results showed that the learnt agent only used a small percentage of their ruleset, to combat this mutation rates could change during a learning run on a rule by rule basis. If a rule goes unused in the evaluation stage, then its mutation rate is increased, if it is used often, then its mutation rate is decreased.

The result that were gathered from the statistical output of the learning module have proved a good analysis of the learnt agent and the process in general. However, it was impossible to properly ascertain key metrics such as population variance and mutation sensitivity. With more time, the fitness and genome of each individual in the learning run would be made available, allowing for a more in depth analysis.


%-----------------------------------------------------------------------
% Project Evaluation
%-----------------------------------------------------------------------

\subsection{Methodology}

Overall the project completed the majority of its objectives and enacted most of the project specification. Its failure to attain all of its aims was centred around two aspects: testing and testability and issues with the Mario benchmark software.

Testing proved more difficult and time consuming than anticipated, due in part to the inclusion of ECJ and the benchmark (see Sections \ref{subsec:agenttest}, \ref{subsec:lptest} and \ref{subsec:learntest}). For several parts of the level playing and learning modules testing was infeasible. This precipitated the abandoning of test driven development (Objective \ref{obj:impl}) and reduced the test coverage (Objective \ref{obj:test}) of the entire project.If the project were to be attempted again, more time and effort would be spend to make the code more testable.

The Mario benchmark software came with several problems (see Section \ref{subsec:enginemod}). Those that were fixed, were time-consuming and caused a schedule overrun. Those that could not be addressed lead to not being able to fully adhere to the project specification (e.g. multithreading the learning process as discussed in Section \ref{subsec:learnrunning}). More research should have gone in the source code at an earlier date, which would have helped identified these issues sooner, allowing preparations to be made.

Due to the modifications of the benchmark software, comparisons to other approaches were limited. Framing the learnt agent in terms of the Mario AI Competition would have been misleading and uninformative as it used an engine that produced much more difficult levels. However, greater effort should have gone toward making these comparisons, especially to the REALM agents. Given more time, the learnt agent would have performed tests in the original engine, allowing score comparisons to the REALM agent and other competition entrants.

Despite these issues, the project met its overarching objective. It successfully created an interesting and capable game-playing agent, developed by evolutionary computation.





