





















font=small,it







	basicstyle=, 
	columns=fullflexible, 
	breaklines=true,
	captionpos=b,                    
  	commentstyle=,    
  	escapeinside=**),          
  	keywordstyle=,       
  	stringstyle=,     
	emphstyle=[1],
	showstringspaces=false



scala
  emph=[1] 
  	Int, Boolean, Vector, Option, Array, Byte, Some, None, Nil, Tuple2
  ,
  morekeywords=abstract,case,catch,class,def,
    do,else,extends,false,final,finally,
    for,if,implicit,import,match,mixin,
    new,null,object,override,package,
    private,protected,requires,return,sealed,
    super,this,throw,trait,true,try,
    type,val,var,while,with,yield,
  otherkeywords==>,<-,<,<:,>:,,@,
  sensitive=true,
  morecomment=[l]//,
  morecomment=[n]/**/,
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""


 images/, diagrams/ 







"@width 1pt



 
 










Birkbeck College
 

Msc Computer Science Project Report



 Learning and Video Games: Implementing an Evolutionary Agent
 




0.4
 
Author:

Monty West 


 
0.4
 
Supervisor: 

Dr. George Magoulas 




MSc Computer Science project report, Department of
Computer Science and Information Systems, Birkbeck College,
University of London 2015



This report is substantially the result of my own work,
expressed in my own words, except where explicitly indicated in
the text. I give my permission for it to be submitted to the
JISC Plagiarism Detection Service.



The report may be freely copied and distributed provided the
source is explicitly acknowledged.




empty
 




empty

Artificial intelligence in video games has long shunned the use of machine learning in favour of a handcrafted approach. However, the recent rise in the use of video games as a benchmark for academic AI research has demonstrated interesting and successful learning approaches. This project follows this research and explores the viability of a game-playing learning AI. Considering previous approaches, an evolutionary agent was created for a platform game based on Super Mario Bros.

The project builds on top of software developed for the Mario AI Competition, which provides the game-engine and agent interface, as well as several other pertinent features. The basic agent was constructed first and a learning framework was built to improve it, utilising a genetic algorithm. The project followed an agile methodology, revisiting design by analysing learning capability.

The aim was to produce an agent that shows meaningful improvement during learning and demonstrated unforeseen behaviours. 

!ADD EVAL HERE!







<Picture mario>  























Introduction

Artificial intelligence (AI) is a core tenant of video games, traditionally utilised as adversaries or opponents to human players. Likewise, game playing has long been a staple of AI research. However, academic research has traditionally focused mostly on board and card games and advances in game AI and academic AI have largely remained distinct.

The first video game opponents were simple discrete algorithms, such as the computer paddle in Pong. In the late 1970s video game AIs became more advanced, utilising search algorithms and reacting to user input. In Pacman, the ghost displayed distinct personalities and worked together against the human player . In the mid 1990s, approaches became more `agent' based. Finite State Machines (FSMs) emerged as a dominant game AI technique, as seen in games like Half-Life . Later, in the 2000s, Behaviour Trees gained preeminence, as seen in games such as F.E.A.R.  and Halo 2 . These later advances borrowed little from contemporary development in academic AI and remained localised to the gaming industry.

In the last ten years, with increases in processing power and the complexity of games, many academic techniques have been harnessed by developers. For example, Monte Carlo Tree Search techniques developed in Go AI research have been used in Total War: Rome II . In 2008's Left 4 Dead, Player Modelling was used to alter play experience for different users . Furthermore, AI and related techniques are no longer only being used as adversaries. There has been a rise in intelligent Procedural Content Generation in games in recent years, in both a game-world sense (for example MineCraft and Terraria) and also a story sense (for example Skyrim's Radiant Quest System) .

However, machine learning has yet to find a meaningful contribution to the world of video gaming, despite being a staple of academic research into AI, especially in robotics and board games. High complexity, small datasets and time constraints greatly hinder the effective implementation of learning techniques in the industry.

Conversely, commercial games have recently enjoyed more consideration in academic research. Games such as Ms. Pac Man, Starcraft, Unreal Tournament, Super Mario Bros. and open-source counterparts TORCS  and Cellz  have been at the centre of recent competitions and papers  . These competitions tend to focus on agent-based game-playing AI, with many entrants adopting an evolutionary learning approach. This research could have applications as AI `competitors' to human players, which is especially relevant to the racing, FPS and RTS genres.

This project will combine the notions of game-playing agents, evolutionary learning and traditional video game AI. By considering similar existing work, an evolutionary agent will be produced that learns to effectively play a 2D platforming game.





Concept Definitions


At this point it is useful to introduce some high level descriptions/definitions of some key concepts that will be used in this report.

Intelligent Agents (IAs)








An intelligent agent is an entity that uses sensors to perceive its environment and acts based on that perception through actuators or effectors. In software, this is often realised as an autonomous program or module that takes its perception of the environment as input an returns actions as output. Figure  shows the basic structure of an intelligent agent. 


Rule-based systems

A rule-based system decides actions from inputs as prescribed by a ruleset or rule base. A semantic reasoner is used to manage to the relationship between input and the ruleset. This follows a match-resolve-act cycle, which first finds all rules matching an input, chooses one based on a conflict strategy and then uses the rule to act on the input, usually in the form of an output. 

Biologically Inspired Learning 

Several computational learning approaches have derived their inspiration from learning in animals and humans. Two such approaches are relevant to this project: Reinforcement learning, strategy modelled on simplistic interpretation of how animal learn behaviour from their environment ; and evolutionary computation, algorithms that apply Darwinian principles of evolution .


Reinforcement Learning

A reinforcement learning agent focuses on a learning problem, with its goal to maximise reward. Given a current state the agent chooses an action available to it, which is determined by a policy. This action maps the current state to a new state. This transition is then evaluated for its reward . This reward often affects the policy of future iterations, but policies may be stochastic to some level. 

Evolutionary Computation

Evolutionary computation encompasses many learning algorithms, two of which are described in detail below. The process looks to optimise a data representation of the problem through random variation and selection in a population. It commonly employs techniques similar to survival, breeding and mutation found in biological evolutionary theory.  
	
Genetic Algorithms (GAs)

Genetic Algorithms are a subset of evolutionary computation. They model the solution as a population of individuals. Each individual has a set of genes (a  genome), which can be thought of as simple pieces of analogous information (most often in the form of bit strings). Each individual is assessed by some fitness function. This assessment can be used to cull the population, akin to survival of the fittest, or to increase the individual's chance of influencing the next population. The new population is created by breeding, using a combination of the following: crossover of the genome from two (or more) individuals (akin to sexual reproduction), mutation of the genes of one individual (akin to asexual reproduction) and re-ordering of the genes of one individual. Each new population is called a generation. 

Evolution Strategies (ESes)

Evolution Strategies are another example of evolutionary computation. They differ from standard Genetic Algorithms by using truncation selection before breeding. The top  individuals of the population are chosen (usually by fitness) and bred to create  children. ES notation has the following form:  .  denotes the number of individuals from  used in the creation of a single , (i.e. number of parents of each child) this report will only consider the case . The  and  are explained below:   

	[]  Denotes an ES that has a population size of lambda. The top  individuals are taken from the  in generation , which then produce  children for generation . This is done by creating  clones of each  and then mutating them individually. 
	[] Differs from the  variant by adding the  individuals chosen from generation  to the new generation  after the mutation phase. Hence the population size is .


Online/Offline Learning

	[Offline] An offline (or batch) learner trains on an entire dataset before applying changes. 
	[Online] A online learner reacts/learns from data immediately after each datapoint.
 





Motivation

Ventures in utilising learning in commercial video games have been limited and largely ineffectual. Game development works on strict cycles and there are limited resources to invest into AI research, especially if the outcome is uncertain. Furthermore, one player playing one game produces a very small data set, making learning from the player challenging. 

However, there are many reasons why good execution of these techniques is desirable, especially biologically inspired learning. Humans must learn and react to environments and scenarios during games, based purely on their perception of the game (and not its inner working). Having non-playable characters do the same may produce a more believable, immersive and relatable AI. Secondly, such learning algorithms produce agents that can respond well in new situations (over say FSMs or discrete logic), hence making new content easy to produce or generate. Lastly, modern games have large and diverse player bases, having a game that can respond and personalise to a specific player can help cater to all. 

Despite the lack of commercial success, video games can act as great benchmark for learning agents. They are designed to challenge humans, and therefore will challenge learning methods, especially those inspired by biological processes. Games naturally have some level of learning curve associated with playing them (as a human). Also, games require quick reactions to stimulus, something not true of traditional AI challenges such as board games. Most games have some notion of scoring suitable for a fitness function. Lastly, they are generally accessible to students, academics and the general public alike.   

As such, games have now been utilised in several academic AI competitions. These competitions are the forefront of research and development into learning techniques in video games, and will be explored in more detail in Section .



Exploring the use of learning techniques for use in video games is a challenging and eminent area of research, with interest from both the video game and computation intelligence communities. This project is motivated by this fact and influenced by the variety of previous approaches taken in these competitions and the unexpected results they produced. 





Aim and Objectives


The aim of the project is to explore the use of behavioural learning techniques in creating a game-playing agent-based AI. This will be achieved by producing an intelligent agent, developed by a evolutionary algorithm, that plays a 2D side-scrolling platform game.

Objectives



	
	Design 

	Influenced by previous approaches, design an agent and learning procedure that can demonstrate meaningful improvement during learning.
	
	
	Implementation 

	Implement agent and learning designs in a modular, customisable and test-driven manner, utilising external libraries and features of the game software.
	
	
	Testing 

	Provide significant test coverage to the functionality of the implementation.
	
	
	Learning 

	Grant the agent a large and diverse testbed from which to learn as well as ample time and resources to do so.
	
	
	Evaluation 

	Evaluate both the competence and learning capability of the agent and compare to alternative approaches.






Methodology

The two central components to this project, the agent and the learning process, will be designed, implemented and tested as two separate modules. This presents a clean separation of concerns. The agent module will be completed first, followed by the learning module. A third module will also be created, which will allow the agent to play generated levels and receive a score.

It is foreseeable that certain parts of the project will be exploratory and as such a redesign of the either the agent or the learning modules may become required. Hence it will be important to take an agile and iterative approach to production, revisiting past objectives if need be.

The open-source Mario benchmark software  (used in the Mario AI Competition) will form the initial codebase for this project. It is written in Java and contains the game-engine and useful features such as level generation and level playing classes.

I intend to use Scala as the main language of this project as I believe that its functional and concurrent features will suit the problem. Scala and Java are widely compatible (as they both compile JVM bytecode) so this should not require a significant amount of work. However, the codebase will be assessed for the suitability of this approach, and Java 8 will be used if Scala proves to be counterproductive.

Furthermore the software will be assessed for the inclusion of build tools, such as sbt and Maven; testing frameworks, such as ScalaMock and Mockito and logging, such as log4j and Scala's inbuilt logging package.

Objective : Design


The design of both the agent and learning module will follow the approach adopted in previous work regarding the REALM agent, which is discussed in detail in Section .

As in previous work, the agent module developed for the project will be a rule-based system that maps environment based conditions to explicit key presses. However, the proposed approach will differ from the REALM agent by exploring several additional perceptions of the available sensory information. These perceptions will be limited to `visible' environment (e.g. the direction mario is moving, presence of enemies etc.). They will be measured and distilled into a collection of simple observations.

The learning module will utilise an offline genetic algorithm as seen in previous work, which will be described later in Section . Agents will be individuals, with rulesets as genomes. Fitness of an individual will be determined by playing generated levels. However, different approaches to mutation, crossover and reordering will be explored, as well as careful consideration of calculating fitness from level playing statistics.

Objective : Implementation


The agent module will conform to the Agent interface included in the benchmark. This will allow the agent to used in other areas of the benchmark software, such as the game-playing and evaluation classes. Each agent will be defined by its ruleset, the contents of which will determine its behaviour. The agent will implement a semantic reasoner for the ruleset, returning a action as prescribed by the chosen rule.

The agent will gather its sensory information from the game's Environment class, which reports visible properties of the level scene. If necessary this class will be extended to include any missing `visible' data. As discussed in , this will be simplified to a list of observations and compared against the agent's ruleset.



The learning module will utilise an external library alongside the level playing module. Many evolutionary computation libraries exist for Java (and therefore Scala), ECJ  and JGAP  will both be considered for use in this project.

The implementation will manage the integration of this library with both the agent and level playing modules. The algorithm will evolve a simple data structure representation of rulesets, inject them into agents and assess those agents by having them play several carefully parametrised levels. The statistics returned from these levels will form the base variables of the fitness function, with multipliers being configurable. To aid in improving the learning process, these parameters will be held externally and fitness values will be logged.

Objective : Testing


Test coverage the agent module will handled by black-box testing. Unit tests will be written to test the semantic reasoner and the environment observation methods.

Due to the stochastic nature of genetic algorithms, testing of the learning module will be limited. However, the breeding can be tested by verifying that children stay within parameters. Some evolutionary libraries, such as JGAP, provide several unit tests. If available these will become the primary source of test coverage for the module.

Objective : Learning


Optimising the learning parameters, including the parameters of the breeding phase, fitness function and level playing, will be an important stage of the project. Assessment of the parameters used in previous agent such as REALM (discussed in ) D. Perez et al.  and Agent Smith  will inform the initial values. From there learning module logs will be analysed for improvements. For example, if there is a lot of variation in fitness then perhaps mutation should be restricted, or if the average fitness does not eventually level out then the further generations should be created.

The design of the agent can also influence the effectiveness of the learning algorithm. The size of the search space is determined by the conditions and actions of the rulesets, the reduction of which could improve evolutionary capability. Hence, the learning phase of the project may inform a redesign of the agent, which is one of the main reasons this project will take an agile approach.

The learning itself is likely to be a time consuming, computationally heavy procedure. To assist in providing ample resources to this process the project will have access to two 8 core servers, as well as a laptop with an Intel i7 processor.

Objective : Evaluation


On the conclusion of the learning process the best/final agent will be extracted and evaluated. This will be done by using the level playing module. The agent will go through an extensive set of levels, based on the approach taken by the 2010 Mario AI Competition.

The primary comparison will be with a handcrafted ruleset, which will assess the significance of the agent evolution. Other comparisons can be drawn against agents that are included as examples in the benchmark, such as the ForwardJumpingAgent that was used for similar comparisons in the 2009 competition, as well as other entrants into the competition .

The second part of this objective is to evaluate the learning procedure. Figures such as average and maximum generation fitness can provide an insight into the effectiveness of the genetic algorithm. Furthermore, a baseline for these values can be provided by having the handcrafted agent play the levels alongside the evolving agent. The final evaluation report will provide an analysis of these figures.




Report Structure

This report will cover previous approaches to the project's aim; the design, implementation of the agent and learning process; and evaluate both the results and the project as a whole.

Section 2 details existing work, specifically relevant entrants to Game AI competitions. It will consider their approaches to both learning and agent design for both effectiveness and significance to learning in video games. It will look in particular at the Mario AI Competition and the winner entrant in 2010, the REALM agent.

Section 3 will cover the projects specification, discussing functional and non-functional requirements. It will also include a description of the major dependencies that influenced project design.

Section 4 will explain the design, implementation and testing of the rule-based agent. It will demonstrate how the agent was built to allow it be evolved by a genetic algorithm, as well as how it perceives its environment and choosing an action. Reasons for the choice of project language and build tools is also included here. 

Section 5 will detail the development of the level playing module.This contains an account of the modifications that had to be made to the game engine software. It covers the extension to the game engine and how it as designed and implemented with a view to parametrisation and level variety.

Section 6 explains the choice of genetic algorithm and the basic parameters used. It also describes extensions made to the learning library to allow it to effectively evolve the agent in an easily customisable and observable fashion. Lastly, it details how specific mutation and fitness parameters were chosen in response to initial learning runs in order to improve the process.

Section 7 presents the data gathered during the learning run(s). Using this data, it studies the effectiveness of the learning algorithm by examining metrics such as fitness increase over generations. It also analyses the learnt agent(s), highlighting interesting behaviour and drawing comparisons to handcrafted agents.

Section 8 evaluates the three major portions of the project. It first considers the agent framework, evaluating it on complexity, speed and capability, and offering possible improvements. It then discusses the positives and negatives of the level playing module, as well as issues that were left unaddressed. It goes on to examine the choice of learning algorithm and parameters, with a view to future revisions and additions. Lastly, it looks at the effectiveness of the project methodology as a whole.


















Existing Work





Learning Agents and Game AI Competitions


Over the last few years several game based AI competitions have been run, over a variety of genres. These competitions challenge entrants to implement an agent that plays a game and is rated according to the competitions specification. They have attracted both academic  and media interest . The competition tend to encourage the use of learning techniques. Hence, several interesting papers concerning the application of biologically inspired learning agents in video games have recently been published. Approaches tend to vary widely, modelling and tackling the problem very differently and specialising techniques in previously unseen ways. 

Some brief details of the competitions which are of relevance to this project are compiled in to Table . The Mario AI Competition is also explored in more detail below.



























The Mario AI Competition


The Mario AI Competition, organised by Sergey Karakovskiy and Julian Togelius, ran between 2009-2012 and used an adapted version of the open-source game Infinite Mario Bros. From 2010 onwards the competition was split into four distinct `tracks'. We shall focus on the unseen Gameplay track, where agents play several unseen levels as Mario with the aim to finish the level (and score highly).  

Infinite Mario Bros.

Infinite Mario Bros (IMB)  is an open-source clone of Super Mario Bros. 3, created by Markus Persson. The core gameplay is described as a Platformer. The game is viewed side-on with a 2D perspective. Players control Mario and travel left to right in an attempt to reach the end of the level (and maximise score). The screen shows a short section of the level, with Mario centred. Mario must navigating terrain and avoid enemies and pits. To do this Mario can move left and right, jump, duck and speed up. Mario also exists in 3 different states, small, big and fire (the latter of which enables Mario to shoot fireballs), accessed by finding powerups. Touching an enemy (in most cases) reverts Mario to a previous state. Mario dies if he touches an enemy in the small state or falls into a pit, at which point the level ends. Score is affected by how many coins Mario has collected, how many enemies he has killed (by jumping on them or by using fireballs or shells) and how quickly he has completed the level. 

Suitability to Learning










The competitions adaptation of IMB (known henceforth as the `benchmark') incorporates a tuneable level generator and allows for the game to be sped-up by removing the reliance on the GUI and system clock. This makes it a great testbed for reinforcement learning. The ability to learn from large sets of diverse data makes learning a much more effective technique. 

Besides that, the Mario benchmark presents an interesting challenge for learning algorithms. Despite only a limited view of the ``world" at any one time the state and observable space is still of quite high-dimension. Though not to the same extent, so too is the action space. Any combination of five key presses per timestep gives a action space of  . Hence part of the problem when implementing a learning algorithm for the Mario benchmark is reducing these search spaces. This has the topic of papers by Handa and Ross and Bagnell  separately addressed this issue in their papers  and  respectively.

Lastly, there is a considerable learning curve associated with Mario. The simplest levels could easily be solved by agents hard coded to jump when they reach an obstruction, whereas difficult levels require complex and varied behaviour. For example, traversing a series of pits may require a well placed series of jumps, or passing a group of enemies may require careful timing. Furthermore, considerations such as score, or the need to backtrack from a dead-end greatly increase the complexity of the problem. 






Previous Learning Agent Approaches


Agent-based AI approaches in commercial games tend to focus on finite state machines, behaviour trees and rulesets, with no learning component. Learning agents are more prevalent in AI competitions and academia, where it is not only encouraged, but viewed as an interesting research topic . Examples from both standpoints are compiled in Table .

Evolutionary Algorithms

In Section  we presented some basic concepts of evolutionary and genetic computing. This approach is a common choice of learning methods used in game-playing agents. D. Perez et al. note in their paper  that evolutionary algorithms are particularly suitable for video game environments: 

`Their stochastic nature, along with tunable high- or low-level representations, contribute to the discovery of non-obvious solutions, while their population-based nature can contribute to adaptability, particularly in dynamic environments.'

The evolutionary approach has been used across several genres of video games. For example, neuroevolution, a technique that evolves neural networks, was used in both a racing game agent (by L. Cardamone ) and a FPS agent (by the UT2 team ). Perhaps the most popular approach was to use genetic algorithms (GAs) to evolve a more traditional game AI agent. R. Small used a GA to evolve a ruleset for a FPS agent , T. Sandberg evolved parameters of a potential field in his Starcraft agent  , City Conquest's in-game AI used an agent-based GA-evolved build plan  and D. Perez et al. used a grammatical evolution (a GA variant) to produce behaviour trees for a Mario AI Competition entry .

Multi-tiered Approaches

Several of the most successful learning agents take a multi-tiered approach. By splitting high-level behaviour from low-level actions agents can demonstrate more a complex, and even human-like, performance. For example, COBOSTAR, an entrant in the 2009 Simulated Car Racing Competition, used offline learning to determine high-level parameters such as desired speed and angle alongside a low-level crash avoidance module . UT2 used learning to give their FPS bot broad human behaviours and a separate constraint system to limit aiming ability .  Overmind, the winner of the 2010 Starcraft Competition, planned resource use and technology progression at a macro level, but used A* search micro-controllers to coordinate units . 

 
One learning agent that successfully utilised both an evolutionary algorithm and a multi-tiered approach is the Mario agent REALM, which is explored in more detail below. 

REALM


The REALM agent, developed by Slawomir Bojarski and Clare Bates Congdon, was the winner of the 2010 Mario AI competition, in both the unseen and learning Gameplay tracks. REALM stands for Rule Based Evolutionary Computation Agent that Learns to Play Mario. REALM went through two versions (V1 and V2), with the second being the agent submitted to the 2010 competition.

Rule-based

Each time step REALM creates a list of binary observations of the current scene, for example ISENEMYCLOSELOWERRIGHT and ISPITAHEAD. Conditions on observations are mapped to actions in a simple ruleset. These conditions are ternary (either TRUE, FALSE or DONTCARE) . A rule is chosen that best fits the current observations, with ties being settled by rule order, and an action is returned .

Actions in V1 are explicit key-press combinations, whereas in V2 they are high-level plans. These plans are passed to a simulator, which reassesses the environment and uses A* to produce the key-press combination. This two-tier approach was designed in part to reduce the search space of the learning algorithm. 

Learning

REALM evolves ruleset using an ES for 1000 generations. The best performing rule set from the final generation was chosen to act as the agent for the competition. Hence, REALM is an agent focused on offline learning. 

Populations have a fixed size of 50 individuals, with each individual's genome being a ruleset. Each rule represents a gene and each individual has 20. Initially rules are randomised, with each condition having a 30, 30, 40 chance to be TRUE, FALSE or DONTCARE respectively.

Individuals are evaluated by running through 12 different levels. The fitness of an individual is a modified score, averaged over the levels. Score focuses on distance, completion of level, Mario's state at the end and number of kills. Each level an individual plays increases in difficulty. Levels are predictably generated, with the seed being recalculated at the start of each generation. This is to avoid over-fitting and to encourage more general rules.

REALM used the  variant ES, with  and  (i.e. the best 5 individuals are chosen and produce 9 clones each). Offspring are exposed to: Mutation, where rule conditions and actions may change value; Crossover, where a rule from one child may be swapped with a rule from another child  and Reordering, where rules are randomly reordered. These occur with probabilities of 10, 10 and 20 respectively . Unfortunately, the method employed to perform these operations is not clearly explained in the REALM paper .

Performance

The REALM V1 agent saw a larger improvement over the evolution, but only achieved 65 of the V2 agent's score on average. It is noted that V1 struggled with high concentrations of enemies and large pits. The creators also assert that the V2 agent was more interesting to watch, exhibiting more advanced and human-like behaviours. 

The ruleset developed from REALM V2 was entered into the 2010 unseen Gameplay track. It not only scored the highest overall score, but also highest number of kills and was never disqualified (by getting stuck in a dead-end). Competition organisers note that REALM dealt with difficult levels better than other entrants. 












































































Project Specification





Functional Requirements
Functionally, the project can be split into three parts: the agent framework, which is responsible for gathering sensory information from the game and producing an action; level generation and playing, which is responsible for having an agent play generated levels with varying parameters; and the learning module, which will apply a genetic algorithm to a representation of an agent, with an aim to improving its level playing ability. 

Agent
The agent framework will implement the interface supplied in the Mario AI benchmark; receive the game Environment and produce an Action. It must be able to encode individual agents into a simple format (e.g. a bit string or collection of numbers). Additionally, it should be able to encode and decode agents to and from external files. 

The framework will be assessed in three ways. Firstly on its complexity, keeping the search space of the encoded agent small is important for the learning process. Secondly on its speed, the agent must be able to respond within one game tick. Thirdly on its capability, the framework must facilitate agents that can complete the easiest levels, and attempt the hardest ones. It is also required that human created agent encoding(s) be written (within the framework) to assist this assessment.

Level Playing
The level generation and playing module must be able to generate and play levels using an agent, producing a score on completion. It should extend the existing framework included in the Mario AI Benchmark software. Furthermore, it should be able to read parameters for generation and scoring from an external file.

The module will be evaluated on the level of variety in generated levels and the amount of information it can gather in order to score the agent.

Learning
The learning module should utilise a genetic algorithm to evolve an agent (in encoded form). It should also ensure that as many as possible of the parameters that govern the process can be held in external files, this included overall strategy as well fine grained detail (e.g. mutation probabilities and evaluation multipliers). Where impossible or inappropriate to hold such parameters externally it must be able to read them dynamically from their governing packages, for example boundaries on the agent encoding should be loaded from the agent package, rather than held statically in the learning module. It must have the facility to report statistics from learning runs, as well as write out final evolved agents.

The learning module will also be assessed on three counts. Firstly, learning should not run for too long, grating the freedom to increase generation count or adjust parameters. Secondly, the learning process should demonstrate a meaning improvement to the agent over generations as this demonstrates an effective genetic algorithm. Thirdly, the final evolved agent will be assessed, using the level playing package. It will tested against the human created agents and analysed for behaviours and strategies not considered during their creation. 






Non-functional requirements

Both the level playing module and the agent framework should not prevent or harm thread safety, allowing multi-threading in the learning module. Each part should be deterministic, i.e. if given the same parameter files will always produce the same results. Lastly, the entire project must have the ability to be packaged and run externally.






Major Dependencies
The project has two major dependencies: a game engine and a learning library. Their selection influenced the design of all aspects of the project and hence are included here. 

As previously mentioned, the project will extend the Mario AI Benchmark. As previously discussed in Section  the Mario AI Benchmark is an open-source Java code-base, build around a clone of the game Super Mario Bros. 3. It was chosen for its aforementioned suitability to learning and for its other pertinent features, including an agent interface, level playing and generation package and other useful additions (e.g. the ability to turn off the reliance on the system clock).

The use of the Mario AI Benchmark restricts the choice of language (discussed further in Section ) to those that can run on the JVM. Hence, ECJ was chosen as the learning library as it is written in Java and is available as both source-code and packaged jar. Furthermore, ECJ fit the project specification very well: it provides support for GAs and ESes, its classes and settings are decided at runtime from external parameter files, it is highly flexible and open to extension, and has facilities for logging run statistics.

Although the intention was to include these dependencies as packaged jars, modification of their source code was necessary (which was available on academic licences). This code was modified in Java, packaged and included as dependencies in the main project. Details of the modifications can found in Sections  and .














Agent Framework





Design
































The benchmark offers an Agent interface, which when extended can be passed into the game playing portion of the software. Every game tick the current Environment is passed to the agent, and then the agent is asked for an Action, which is a set of explicit key press for Mario to perform.

The agent framework's basis will be extending this interface. Similar to REALM's agent, it will be a rule-based system. A UML class diagram of the system is given in Figure .



Each Agent instance is initialised with a ruleset, consisting of a list of rules, each containing an Action. On receiving the current game Environment it is passed to a number of perception objects. Each one is responsible for a exactly one measurement (e.g. Enemy ahead, Mario moving to right etc.). These measurements are collected in to an observation object and stored in the agent.

When asked for an action, the Agent passes this observation to its ruleset, which tests it against each rule. Rules contain conditions on what perception measurements should be, which determine its score for a given observation. The highest scoring rule (with a conflict strategy for ties) is then asked for its action, which is returned from the agent.



Perceptions are hard coded; as such, all of an agents behaviour is determined by its ruleset. Thus, an implementation of the ruleset which allows persistence ensures that agents can be read and written to external files. Furthermore, rulesets are immutable and only information received during the current game tick is considered. Hence, agents have no state and are deterministic and thread safe (assuming the game engine is).

The capability of this framework is closely tied to the choice of perceptions. The more detail an agent is able to observe in its environment, the more potentially capable the agent can be. However, increasing the number of perceptions, and the detail they perceive at, increases the search space, making capable agents less likely to result from random mutation. Therefore, selection of perceptions must be careful considered, with a balance being struck between detail and brevity. 

The perceptions chosen were based upon those used in the REALM v1 agent . The following is the final list of perceptions chosen:

[MarioMode] Measures whether Mario is small, big or fire (capable of throwing fireballs).
[JumpAvailable] Detects if Mario has the ability to jump (i.e. on the ground or gripping a wall with the jump key off).
[OnGround] Measures whether Mario is in the air or not.
[EnemyLeft] Detects if an enemy is to the left of Mario.
[EnemyUpperRight] Detects if an enemy is to the upper right of Mario.
[EnemyLowerRight] Detects if an enemy is to the lower right or directly to the right of Mario.
[ObstacleAhead] Detects if there is a block or terrain directly to the right of Mario.
[PitAhead] Detects if there is a pit (falling into pits will end the level) to the right of Mario.
[PitBelow] Detects if there is a pit directly below Mario.
[MovingX] Measures the direction Mario is moving horizontally.
[MovingY] Measures the direction Mario is moving vertically.


The set of actions Mario can take is any combination of 6 key presses: left, right, up, down, jump and speed (which also shoots a fireball if possible). It was decided that this set would be restricted for use in the ruleset agent, only allowing cobinations of left, right, jump and speed. This was to reduce the size of the rules, which reduces the search space of any learning process. The up key performed no function and the down key made Mario crouch, which seemed too niche to be useful during the design process.






Language and Tools

Scala

The dependency on the Mario AI Benchmark restricts the language choice to those that can run on the JVM. Hence, Scala was chosen to be the primary language of the project. Its functional programming elements (pattern matching, list processing etc.) are very applicable to the ruleset and its semantic reasoner. Scala's focus on immutability aids in maintaining the thread safety requirement. Furthermore, ECJ's structure necessitates the use of type casting, which Scala handles elegantly. Several other Scala features were used throughout the project, such as lambdas, singletons, type aliases and case classes. 

Maven
With two major and several minor dependencies, their management is important to the project. Maven was chosen to maintain this, as well as package the main project and both the sub-projects. It was chosen for its ability to package both Java and Scala projects, keeping the tools consistent across the entire code base.






Implementation








The design of the agent framework allows for agents to be fully determined by their ruleset. In order for an agent to be used in a evolutionary algorithm it must be represented by a simple data structure. Hence, implementation must allow for agent's rulesets to be unambiguously represented as, for example, a one dimensional collection. This section will describe the implementation steps taken to represent rulesets as a array of 8-bit integers, utilising Scala's built in types Vector and Byte.

Perceptions

The Perception interface was implemented as an abstract class, with an index integer field and an apply method. apply takes the game Environment and returns a Byte (an 8-bit integer) instance, representing the measurement. Perception was extended into two further abstract classes: BoolPerception, which enforces apply's return be either 1 or 0, representing true and false respectively; and BytePerception, which has a limit field and enforces apply's return is between 0 and limit (inclusively). A full list of perceptions and what their byte values represent can be found in Table  in Appendix .

Concrete perceptions were implemented as objects (singletons) using the case keyword, which allows for exhaustive pattern matching on the Perception class, as well as type safe extension by adding further case objects. Each one declares a unique index integer to the Perception superclass (starting at 0 and increasing by one for each). They implement the apply method to return their specific measurement. Figure  shows a class diagram of the this system.

For illustration, consider these examples. The perception PitBelow extends BoolPerception, with a unique index of 8 and implements apply to return 1 if there is a pit directly below Mario and 0 otherwise. MarioMode extends BytePerception, with a unique index of 0 and a limit of 2, with apply returning 0 if Mario is small, 1 for big and 2 for fire.

This approach keeps all the information about perceptions contained in one file. The number of perceptions is easily extended by adding more case objects. Furthermore, it allows the Observation and Conditions classes to be implemented as fixed length byte vectors, with the vector's index matching the perception's unique index field. With use of Scala's implicit and extractor functionality, building the Observation and validating the Conditions vectors is type safe and concise:


0.9

[language=scala]
val observationVector = 
    Vector.tabulate(Perception.NUMBER_OF_PERCEPTIONS) 
        n: Int => n match 
            // This retrieves the perception object with the index
            // that matches n. 
            case Perception(perception) => perception(environment)
        
    



0.9

[language=scala]
def validateConditions(conditionsVector: Vector[Byte]): Boolean = 
    conditionsVector.zipWithIndex.forall 
        case (b: Byte, Perception(perception)) => perception match 
            case boolP : BoolPerception => 
                (b == boolP.TRUE)  (b == boolP.FALSE) 
                     (b == DONT_CARE)
            case byteP : BytePerception => 
                ((0 <= b) && (b <= byteP.limit))  (b == DONT_CARE)
        
        case _ => false 
    





Notice that no information about specific concrete perceptions is required, enforcing the open-closed design principle and allowing perceptions to be added without need to alter this code.

Perceiving the Environment

The Environment interface contains several individual methods that report Mario's situation. Therefore implementing Perceptions that concerned Mario (e.g. MarioMode, MovingX etc.) was trivial.

Perceptions pertaining to enemies, obstacles or pits was more challenging. Environment provides two 19x19 arrays, one for enemies and one for terrain. Centred around Mario, each array element represents a `square' (16 pixels) of the level scene. The value of an element marks the presence of an enemy or terrain square. 

Enemy and obstacle perceptions pass the relevant array, a lambda test function, coordinates for a box segment of the array and a boolean to a helper function. This function uses Scala's for loop comprehensions to search through the box, applying the lambda to each element, returning the boolean parameter if the lambda returns true at least once. In this way it is easy to search for an enemy or obstacle in a box relative to Mario. Pits work in a similar way, but declare columns instead (see Listing  in Appendix ). If there is no terrain in the column below Mario's height, then it is considered a pit. Take EnemyLeft for example:

0.9

[language=scala]
case object EnemyLeft extends BoolPerception(3) 
    // Minus = (Up,Left)  Plus = (Down,Right)
    val AREA_UL = (-2,-2); val AREA_BR = (1, -1);
  
    def apply(environment: Environment): Byte = 
        if(Perception.enemyInBoxRelativeToMario(environment,
            AREA_UL, AREA_BR)) 1 else 0  
    




0.9

[language=scala]
def enemyInBoxRelativeToMario(environment: Environment, 
          a: (Int, Int), b: (Int, Int)): Boolean = 
    val enemies = environment.getEnemiesObservationZ(2);
    val test = (grid: Array[Array[Byte]], tup: Tuple2[Int, Int]) =>
         val x = grid(tup._1)(tup._2)
         x == 1
    
    checkBox(enemies, test, getMarioPos(environment), a, b, true)




0.9

[language=scala]
def checkBox(grid: Array[Array[Byte]], 
               test: (Array[Array[Byte]], (Int, Int))=>Boolean, 
               mario: (Int, Int), 
               a: (Int, Int), b: (Int, Int), 
               ret: Boolean): Boolean = 

    val relARow = min(grid.length-1, max(0, (a._1 + mario._1)))
    val relACol = min(grid(0).length-1, max(0, (a._2 + mario._2)))
    val relBRow = min(grid.length-1, max(0, (b._1 + mario._1)))
    val relBCol = min(grid(0).length-1, max(0, (b._2 + mario._2)))



0.9

[language=scala]   
    for 
      i <- min(relARow, relBRow) to max(relARow, relBRow)
      j <- min(relACol, relBCol) to max(relARow, relBCol)
      if (test(grid, (i, j)))
     return ret 
    !ret





Observation, Conditions and Actions

For clarity an Action class was created for use in the ruleset, with an adapter method to convert it into the boolean array expected in the Agent interface. As the Observation and Conditions classes were to be implemented as fixed length byte vectors, so was the Action class.

Action vectors have a fixed length of 4, where elements represent left, right, jump and speed respectively. Observation vectors have a fixed length equal to the number of perceptions and hold the byte returned by each Perception's apply function. Conditions have the same length and hold data in the same range as Observation, the condition on each perception therefore being that the corresponding element in the observation be equal. They have one additional possible value, DONTCARE (equal to byte -1), which represents that no condition be placed on that perception.

Instead of implementing these classes as wrappers for Vector[Byte], which can be inefficient and overly verbose, type aliases were used. This allowed each class to be referred to explicitly (rather than just by variable name), which provides readability and type safety, whilst still having direct access to the list processing methods included in the Vector class. They were declared on the agent framework's package object, making them accessible package wide. An object with static data and factory methods was included for each.

For example, this allowed Observation to be used as such:

0.9

[language=scala]
abstract class ObservationAgent extends Agent 
    // Using factory method for a blank observation
    var observation: Observation = Observation.BLANK
    ...
    def integrateObservationenv: Environment: Unit = 
        // Using the Observation factory method 
        // to build a new observation
        observation = Observation(env)
        if (printObservation) 
            // Using Vector method foreach directly
            observation.foreach  
                b:Byte => print("   " + b)
            
        
        ...
    
 





Rules and Rulesets

Having both Conditions and Action implemented as byte vectors allows rules to be represented in the same way. Each rule is simple the concatenation of the Conditions and Action vectors. The rule vector are fixed length as both Conditions and Action are. Moreover, as rulesets contain just a list of rules, rulesets can be unambiguously represented by a single dimension byte vector. This allows rulesets not only to be persisted easily (as say a csv) but also gives the data representation needed for the evolutionary process.

In this case both Rule and Ruleset were implemented as wrapper classes for Vector[Byte] and Seq[Rule] respectively. Ruleset also holds a default Action, which is used if no rule matches the environment.

The semantic reasoner of the rule system is split across both classes. In Rule, the scoreAgainst method is passed the observation and produces a score by looping through the conditions and adding 1 if the condition and observation match in value and 0 if the condition is DONTCARE. If a mismatched condition is found, the method immediately returns with -1. It is implemented tail recursively to provide maximum efficiency.

0.9

[language=scala]
def scoreAgainst(observation: Observation): Int = 
    val conditions = ruleVector.slice(0, Conditions.LENGTH)
    @tailrec
    def scoreRecu(i: Int, sum: Int = 0): Int =  
        if (i == Conditions.LENGTH) sum
        else conditions(i) match 
            case Conditions.DONT_CARE => scoreRecu(i+1, sum)
            case b if b == observation(i) => scoreRecu(i+1, sum+1)
            case _ => -1
        
    
    scoreRecu(0)




In Ruleset, the getBestAction is passed the observation and returns an action boolean array. Using tail recursion it performs a fold operation on its rules, saving and returning the best scoring rule (preferring earlier rules when tied). If no rule gets a score of 0 or above then the default action is returned.

0.9

[language=scala]
def getBestExAction(observation: Observation): Array[Boolean] = 
    
    @tailrec
    def getBestRuleRecu(rs: Seq[Rule], best: Option[Rule] = None, bestScore: Int = 0): Option[Rule] = 
        rs match 
          case Nil => best
          case (r +: ts) => 
              val newScore = r.scoreAgainst(observation)
              if (newRuleBetter(bestScore, newScore))
                  getBestRuleRecu(ts, Some(r), newScore)
              else
                  getBestRuleRecu(ts, best, bestScore)
        
    


    
0.9

[language=scala]
    getBestRuleRecu(rules) match 
        case None => defaultAction.toBooleanArray
        case Some(r) => r.getAction.toBooleanArray
    





Persistence

Agent's are persisted by persisting their ruleset. Rulesets are persisted in single line csv files. An IO helper object is passed an agent, extracts its ruleset and requests its vector representation, writing each byte separated by a comma. On reading an agent file, it constructs a byte vector. This byte vector is passed to the Ruleset's factory method, which groups the vector by rule length to form the rule sequence.





Testing


Due to the agent modules heavy reliance of the Environment interface the use of a mocking facility was required. The ScalaMock testing library was adding to the project to provide this.

Perceptions were unit tested individually, using white-box approach (due to the inclusion of mocking). Each test stubbed the Environment interface, instructing it to return a specific value (or array) for the relevant call, and testing that the perception echoed or processed it correctly. This allowed Perceptions to be tested independently of the game engine and provided test coverage. However, as there was very little documentation Environment interface, expected return values had to be investigated manually and edge cases could have easily been missed.

Rulesets (and Rules) were tested with a largely black-box end-to-end style. This was required due to the reliance of type aliases. Fixed rulesets were constructed to verify that the getAction method returned the expected action based on a fixed observation. Mocking of individual rules was not used in case the Rule class was altered to be a type alias instead of a wrapper class.

These tests were added to Maven's build lifecycle, and hence run on every project build.





Handcrafted Agents


For the purpose of evaluation and comparison three handcrafted rulesets were created for the agent framework. Full rulesets for these agents can be found in Appendix .


	[Forward Jumping] This ruleset commands the agent to jump whenever it can, regardless of its surroundings. It contains a single rule and a default action. It is a blind agent that does not take advantage of the framework, however it is surprising effective. An analogous agent was used for comparisons in the 2009 Mario AI Competition and was found to score higher than many entrants . The learning process will aim to discourage this behaviour as it is neither interesting or optimal.
	[Simple Reactive] This agent only jumps when it detects an enemy, obstacle or pit in its way. It contains 5 rules and defaults to moving right at speed. This agent makes better use of the framework, but still does not use all of its perceptions. Its behaviour is more interesting, however it tends to score similarly to the forward jumping agent. Despite low attainment, a learnt agent that behaves in a similar way will be evaluating more favourably as it is using more of the agent's perceptions.
	[Complex] This agent is the most interesting and highest scoring of the three. It has several different behaviours and builds off of the simple reactive agent. It contains 18 rules and makes use of all perceptions except MarioMode and EnemyLeft. Its behaviour was investigated at length and as such will form a good comparison to the final learnt agent. Effective evolved behaviours unconsidered in this agent's creation are a sign of the validity of the learning process. 

Creation of the handcrafted rulesets also informed additions and alterations to the agent's perceptions. For example, it was originally difficult to create an effective pit strategy and so PitAhead was changed from a BoolPerception to a BytePerception, returning 3 values representing none, far and close.
















Level Playing Module

The main purpose of the level playing module is to evaluate the fitness of agents during the learning process. An effective learning process needs a diverse testbed, thus the level playing module must be deterministic, highly configurable and able to provide variety. 





Design








In the benchmark the heart of the game engine is the MarioEnvironment class. It is responsible for calling the LevelGeneration class, updating the scene each tick with an action, and reporting the scene through the Environment interface. Parameters controlling level generation and playing are contained in the MarioAIOptions class. The BasicTask class controls the game loop in conjunction with an agent. It initialises the MarioEnvironment class with options, then runs through the loop, commanding a tick, passing the environment to the agent and finally requesting an action and passing it to the MarioEnvironment instance. Statistics pertaining to the agents performance are stored in MarioEnvironment in the EvaluationInfo class, which are cloned to BasicTask at the end of the level. Fitness can be requested from EvaluationInfo with the optional parameter, a SystemOfValues instance, which contains the multipliers for various measurements.

The MarioAIOptions class contains several useful parameters. They include: a integer seed, which is passed to the level generator's RNGs; a boolean that toggles visualisation; integers that determine level difficulty, length and time-limit; and booleans that toggle enemies, pits and blocks. The SystemOfValues class contains multipliers for  distance achieved, whether or not the level was completed, the amount of time left on completion as well as many others.



The level playing module will extend this system. Its primary objective will be to allow for multiple levels (episodes) to be played with options that update as prescribed by some parameter class. Upon completion it will produce a fitness based on all levels played. Furthermore, it will allow for the injection of different agents and level seeds, to allow different agents to play the same options without rebuilding the instance. A UML class diagram for the module can be found in Figure .

Level options are stored in the MWLevelOptions class, which acts as an adapter for the MarioAIOptions class. Each new level these will be updated by the dedicated MWLevelOptionsUpdater class. The MWEvaluationMultipliers class is an adapter for the SystemOfValues class, which is used to calculate the fitness at the end of a sequence of levels. These classes are separated from the the game playing task and are included using constructor dependency injection. This helps to decouple the system and improve the ability to modify and test.

Both MWLevelOptions and MWEvaluationMultipliers are designed to be data classes, which provides determinism and thread safety, as well as easy persistence. MWLevelOptionsUpdater's qualities in this regard are Implementation details and are discussed in a following section. A full list of field for both the MWLevelOptions and MWEvaluationMultipliers classes can be found in Appendix .






Implementation






































Parameter Classes


The MWLevelOptions and MWEvaluationMultipliers classes were implemented as data classes in an immutable builder pattern style. Each field has a withField(field) method that returns a cloned instance with that field changed. This affords a concise, declarative style, whilst maintaining immutability. MWEvaluationMultipliers has a implicit converter to a SystemOfValues instance, which is required for evaluation in the benchmark. However, a similar approach was not possible for converting MWLevelOptions to a MarioAIOptions instance (required for the game-engine). MarioAIOptions hold more parameters than MWLevelOptions (e.g. agent and level seed), therefore the adapter function takes both the current MarioAIOptions and a MWLevelOptions instance as parameters and updates the former by overriding the corresponding fields with values from the latter.

MWLevelOptions was not implemented as a class. Instead, MWBasicTask expects a lambda function. This lambda takes the episode number and the current MWLevelOptions, returning an updated set of options. MWLevelOptions builder structure ensures this is always a new instance, and hence maintains immutability. The inclusion of the episode number allows the function to remain deterministic. Moreover, with the ability to build closures in Scala, this lambda can be built from data (for example, a list of options, where indexes relate to episode number).

Persistence


The primary use of the level playing module is during the evaluation stage of the learning process. Hence, it was decided to use ECJ's parameter file system to persist level playing parameters, which allows them to be written in the same file as the rest of the learning parameters.

ECJ's parameter system builds upon Java's Properties file system. From a parameter file (formatted in the Java Properties format) a ParameterDatabase can be built, from which specific parameters can be requested using the Parameter class. This system was used to persist the two parameter classes, MWLevelOptions and MWEvaluationMultipliers; the level options update lambda data; and other level playing data such as number of levels (episodes) and base level seed. For example, the following lines would set the number of levels to 10 and the base difficulty to 5:

0.9


          level.num-levels = 10
level.base.difficulty-num = 5



A static utility class EvaluationParamsUtil was created to handle the reading of the level playing data from these files. A ParameterDatabase is built and passed to utility class, which builds the required parameter class. For MWLevelOptions and MWEvaluationMultipliers it searches for the prefixes `level.base' and `mult' respectively and then looks for suffixes corresponding to specific fields. If a field's suffix is not found then it is initialised to a default value (which is always zero for MWEvaluationMutlipliers).

The update lambda is built as a closure on a collection of Map instances, one for each field in MWLevelOptions. For each n from 0 to the number of levels (episodes), the utility function looks for the prefix `level.n', which is used to hold the update for episode n. For each field a Map is built and using the same suffixes as for MWLevelOptions, key-value pairs are added mapping n to the value found. When the update lambda is called, it consults these maps and updates the MWLevelOptions with the new value if one is found for the current episode number.

For example, if the parameter file contained the following lines:

0.9


   level.num-levels = 4
 level.base.enemies = false
    level.1.enemies = true
    level.3.enemies = false



Then enemies would be off for the first episode, on for the second and third and off again for the fourth and final episode.

The Episode Loop

The entry point of the level playing module is the MWEvaluationTask class, which extends the abstract MWBasicTask class. MWEvaluationTask is instantiates with a base set of options (as MWLevelOptions), a MWEvaluationMutlipliers instance and an update lambda, as well as the number of episodes (levels) to run. The agent and base level seed can be injected with the withAgent(agent) and withLevelSeed(seed) methods (which also reset of evaluation information).

0.9

[language=scala]
class MWEvaluationTask(val numberOfLevels: Int, 
                       val evalValues: MWEvaluationMultipliers, 
                       override val baseLevelOptions: MWLevelOptions, 
                       override val updateOptionsFunc: (Int, MWLevelOptions) => MWLevelOptions)
                           extends MWBasicTask("MWMainPlayTask", baseLevelOptions, updateOptionsFunc, visualisation, args) with EvaluationTask 

     
    
0.9

[language=scala]  
    private var baseLevelSeed: Int = 0;
    
    override def nextLevelSeed(episode: Int, lastSeed: Int) =  
        (3*episode) + lastSeed
        
        
    override def evaluate: Int = 
        doEpisodes
        localEvaluationInfo.computeWeightedFitness(evalValues)
    
  
    override def withAgent(agent: Agent): MWEvaluationTask = 
        super.injectAgent(agent, true)
        this
    

     
    
0.9

[language=scala]   
    override def withLevelSeed(seed: Int): MWEvaluationTask = 
        baseLevelSeed = seed
        super.injectLevelSeed(seed, true)
        this
    


  

On calling the evaluate() method, the number of levels is passed to the doEpisodes(numberOfEpisode) (a superclass method), which loops as follows:

0.9

[language=scala]

def doEpisodes(amount: Int): Unit = 
    @tailrec
    def runSingle(iteration: Int, prevOptions: MWLevelOptions, disqualifications: Int): Int = 
        if (iteration == amount)  
            disqualifications
         else 
            // Calls the update lambda to get episodes set of options
            val newOptions = updateOptions(iteration, prevOptions)

     
    
0.9

[language=scala]              
            // Converts options to class required for game-engine
            val marioAIOptions = MWLevelOptions.updateMarioAIOptions(super.options, newOptions)
            
            // Updates the level seed (which is set to increase each episode by MWEvaluationClass)
            // Agent instance is already being held here
            marioAIOptions.setLevelRandSeed(nextLevelSeed(iteration, marioAIOptions.getLevelRandSeed))
            
            // Resets the evaluation information in the super class
            super.setOptionsAndReset(marioAIOptions)



0.9

[language=scala]
            // Generates and runs the level using the super class
            // Returns true if the agent was not disqualified (took too long to return an action)
            val notDisqualified: Boolean = runSingleEpisode(1)
            val disqualification: Int = if (!notDisqualified) 1 else 0
       
            // Update the evaluation information for the entire run
            // (as the super classes evaluationInfo gets reset every level
            updateLocalEvaluationInfo(super.getEvaluationInfo)
            
            // Loop
            runSingle(iteration+1, newOptions, disqualifications + disqualified)
        
    
    
    // Sets the base options
    super.setOptionsAndReset(MWLevelOptions.updateMarioAIOptions(options, baseLevelOptions))
    disqualifications = runSingle(0, baseLevelOptions, 0)





Before every episode the options are updated by the lambda, updating the base options in the first episode. A new level seed is also requested from the MWEvaluationClass, which simply increases it each episode. These updates are converted and added to a MarioAIOptions instance and passed to the superclass in the benchmark. A single level is then generated and played using the superclass. Evaluation information is added to MWBasicTask's local evaluation information (as the former is reset every episode) and the function loops tail recursively.

When the number of loops equals the number of levels the function exits. At this point the evaluate() method requests a fitness from the local evaluation information, passing in the evaluation multipliers, which is then returned.





Modifying the Benchmark


Preliminary runs of the benchmark software revealed several issues and defects. As the learning algorithm would run over several generations, any errors could halt it prematurely, which could be costly in terms of project time keeping. In order to address this a Java project was created from the benchmark's source code and fixes were made. This code was packaged with Maven and included as a dependency in the main project.

Several minor exceptions were caught or addressed, however the two largest issues failed `quietly'. They concerned the LevelGeneration class and surrounded enemy and pit generation in regards to level difficulty. Fixes were made to ensure level difficulty scaled more consistently.

Enemy Generation

In observing the benchmark generated levels it was apparent that enemy density was very high, even on lowest difficulties. Examining the LevelGeneration class revealed this to the result of what was probably unintended behaviour. 

Levels are generated in zones of varying length. Quite often, a zero length zone is created, which has no effect on terrain. However, enemies were still being added to these zones, creating very high density columns of enemies during levels. This was addressed, with the addition of a more gentle enemy count curve and better spacing.

Pit Generation

Another apparent shortcoming was pit length. Pits were only of two sizes, small and very large, and after a certain level difficulty were always very large. Although this was intended behaviour, comments from the original developers suggest it was a placeholder for a more sophisticated system. An edit was made to scale maximum pit length on level difficulty. Each pit's length is chosen probabilistically on a bell curve, which is shifted by level difficulty.






Testing


The benchmark's BasicTask class is tightly coupled to the MarioEnvironment, which means that we were unable to mock the game-engine for testing. This problem extends to the MWEvaluationTask class.

However, the decoupling of the parameter classes from MWEvaluationTask means that the persistence section of the module can easily be tested. The EvaluationParamUtil class is white-box unit tested by stubbing the ParameterDatabase interface and verifying the contents of the parameter classes requested. For example:

0.9

[language=scala]
"getEvaluationMutlipliers" should "return eval mults from databade and zero otherwise" in 
    val base = blankParam.push(EvaluationParamsUtil.P_EVAL_BASE)
    (pdStub.exists _) when(base, *) returns(true)
  
    (pdStub.exists _) when(base.push(EvaluationParamsUtil.P_COINS), *) returns(true)
    (pdStub.getIntWithDefault _) when(base.push(EvaluationParamsUtil.P_COINS), *, *) returns(10)
     
    (pdStub.exists _) when(base.push(EvaluationParamsUtil.P_KILLED_BY_SHELL), *) returns(true)
    (pdStub.getIntWithDefault _) when(base.push(EvaluationParamsUtil.P_KILLED_BY_SHELL), *, *) returns(200)

     
    
0.9

[language=scala]       
    (pdStub.exists _) when(base.push(EvaluationParamsUtil.P_DISTANCE), *) returns(true)
    (pdStub.getIntWithDefault _) when(base.push(EvaluationParamsUtil.P_DISTANCE), *, *) returns(1)
    
    val evalMults = EvaluationParamsUtil.getEvaluationMutlipliers(pdStub, base);
    
    assert(evalMults.coins == 10)
    assert(evalMults.killedByShell == 200)
    assert(evalMults.distance == 1)
    assert(evalMults.win == 0)
    assert(evalMults.kills == 0)
    ...










Comparator Task


In order to quantifiably compare agents a competitive set of evaluation task options was created, modelled on those used during the final evaluation stage of the 2010 Mario AI Competition.

Agents play 512 levels, spread equally over 16 difficulty levels (0 to 15). Options such as enemies, pits, blocks etc. are periodically turned off for a level. Length is varies greatly from level to level, with the time-limit being adjusted accordingly. Evaluation multipliers reward all possible positive statistics, such as enemy killed, coins collected and distance travelled. A full view of the comparator task's parameter classes can be found in Appendix .

The scores and other statistics attained by the three handcrafted agents playing the evaluation task, on seed 10, can be found in Table .
































Learning

The learning module's goal is to utilise the ECJ library to evolve a population of byte vectors (representing agent rulesets). This is achieved through extending ECJ's classes, in conjunction with a parameter file. The ECJ library is an extensive collection of learning processes and, due to the limitations of this report, only the relevant sections, and the extensions thereof, are described. An important part of the project was adjusting the parameter file in reaction to previous learning runs, these revisions are discussed in Section . However, some parameters were deemed fundamental and stayed invariant throughout this process; these are described, alongside the general evolutionary approach, in the following section.





Evolutionary Approach


Similar to the approach taken in developing the REALM agent (described in Section ), this project uses a  ted evolution strategy. However, unlike REALM, it focuses purely on mutation, with no crossover between individuals (a technique not traditionally applied with ESes ).

Evolution runs for 1000 generations with a population size of 50. The truncation selection size for our ES () is set to 5. Hence, the top 5 fittest individuals are chosen each generation. These are cloned and mutated to form 45 () new individuals, which are then joined by the original 5 for the next generation.

Each individual's genome is a byte vector of length 300, representing a ruleset containing 20 rules. The default action is fixed over all rulesets and is specified by the parameter file.

Each element of the genome (a gene) has several mutation parameters associated to it. During the mutation phase each gene will be mutated with the probability prescribed by its mutation probability. Each gene can be mutating in one of two ways. The first will simply choose a random byte between its minimum and maximum values (inclusive), known as reset mutation. The second will favour one particular byte value, with a certain probability, which is known as favour mutation. Which method of mutation a gene undergoes, as well as the favoured byte and its probability are all contained in the individual gene's parameters, allowing for very fine grained control over the mutation process.

The evaluation stage takes each individual's genome and constructs an agent. The individual's fitness is calculated by passing the agent to the MWEvaluationTask, which has been initialised with a fixed set of level playing parameters decreed by the parameter file. These parameters have a custom set of evaluation multipliers and describe a task of 10 levels, varying level options for each level. Each generation builds this task with its own unique level seed to avoid over-fitting to one set of generated levels.

Statistics for average and best fitness are logged each generation. Level seed is also reported, to allow direct comparison between the learning process and the hand-crafted agents. After the final generation a selection of agents are written to agent files, including the best individual of the final generation and the best individual over all generations.





The ECJ Library























The primary method for specialising ECJ for a particular use case is to extend the classes it provides, and specify these subclasses, alongside other pertinent values, in the parameter file. Nearly every method in ECJ's classes is passed the pseudo-global EvolutionState singleton, which holds instances of all other classes in use. A top-level view of ECJ's structure can be found in Figures  and .

The learning module will be built on top of the ECJ library, with a view to adhering to its style and ideology. For our approach, several of ECJ's classes did not need to extended, and simply specifying their default implementations in the parameter file was enough. The classes of interest to this project are the following: Species, Individual, SelectionMethod, Breeder, BreedingPipeline, Evaluator, Problem and Statisitics. A simplified class diagram of the learning module is included as Figure .

ECJ provides the ByteVectorIndividual class, which contains a Java array of bytes, this is used to hold individual byte vector genomes. It also decrees the use of the IntegerVectorSpecies class, which holds the gene parameters (e.g. minimum value, mutation probability). Our design first extends this class to the DynamicParameterIntegerVectorSpecies class, which holds a subclass instance of another new class, the DynamicSpeciesParameters class. The specific subclass to be used is specified in the parameter file. The purpose of this class is to provide a facility to override the min, max and mutation probability properties of each gene directly from the agent framework (rather than the parameter file). This class is then further extended to the RulesetSpecies class, which holds gene parameters pertaining to favour mutation. This class also extends the parameter file's vocabulary to allow gene parameters to specified on a condition or action (the composite parts of a rule) basis. For example, the favour byte for all genes that represent conditions can be set as follows in the parameter file:

0.9


   ...species.condition.favour_byte 	= -1




The SelectionMethod and Breeder classes are responsible for selecting individuals (based on fitness) to clone and pass to the breeding pipeline. ECJ natively supports evolution strategies and so it is enough to specify these classes to be ECJ's ESSelection and MuPlusLambda classes. The BreedingPipeline extension, RulesetMutationPipeline, performs the mutation of individuals. For each gene in a individual's genome it requests the corresponding mutation parameters from the species class. It then clones and, if required, mutates the gene with either favour or reset mutation. 

The Evaluator class is responsible for threading and batching the individuals to have their fitness calculated by cloned instances of the Problem class. Hence, only the Problem class is extended, to the AgentRulesetEvaluator class. On initialisation this class reads the the level options from the parameter file using the EvaluationParamsUtil class described in Section . Every generation, agents are constructed from the individuals of the population and passed, with the options and the level seed, to the MWEvaluationTask class. This returns a score, which is attached to the individual as its fitness.

Generational statistical reporting is handled by an extension of the Statistics class, which will output to log files specified in the parameter file.







Implementation

Extensions to the ECJ library were implemented in Scala, organised into a package and situated in the same project as the agent framework and level playing modules. This was to allow the level playing parameters to be read through ECJ's ParameterDatabase class (described in Section ).



Nearly every class in the ECJ library implements a setup(...) method, which is used initialise an instance. This method is passed the EvolutionState singleton, from which it can access the ParameterDatabase, which holds the contents of the parameter file. Classes can poll the ParameterDatabase by constructing a Parameter instance (in a composite pattern) with string keys.

Modifying the ECJ Library


During the implementation process it became clear the the proposed species design was problematic due to the setup process of IntegerVectorSpecies and its superclass VectorSpecies. The setup method reads in values for gene minimum, maximum and mutation probability and then calls for the Individual prototype to be built. This means that any subclass of IntegerVectorSpecies cannot decide the values of the gene minimum, maximum and muation probability. If they are written before calling setup on the superclass (which is required) they will be overridden, if they are written after the calling setup on the superclass they will not be built into the prototype. To address this the VectorSpecies class was modified to include the prePrototypeSetup method, which is called immediately before constructing the prototype in the setup method. Subclasses can extend this method to overwrite any properties set by IntegerVectorSpecies and VectorSpecies.

0.9

[language=java]
public void setup(final EvolutionState state, final Parameter base) 
    // Add min, max, mutation probability etc.
    ...

    // Allow subclasses to override values
    prePrototypeSetup(state, base, def);
    state.output.exitIfErrors();          

    // NOW call super.setup(...), which will in turn sets up the prototypical individual
    super.setup(state,base);


// new method added
protected void prePrototypeSetup(final EvolutionState state, final Parameter base, final Parameter def) 
    //None by default, for subclasses




Species

IntegerVectorSpecies and VectorSpecies store gene parameters (e.g. min, max, mutation probability) as arrays of equal length to the genome, matching by index. These arrays are built during the setup method by polling the ParameterDatabase for the suffixes `min-value', `max-value' and `mutationprob'. The parameter file can specify them globally, in segments or by individual gene (by index). For segments and single indexes the loadParameterForGene method is used, which takes the gene index and a prefix parameter (e.g. segment number or index). It builds a Parameter instance by appending the gene param suffixes to the passed prefix. It requests the Parameter's value from the ParameterDatabase. If a value is found, it sets it as the element of the corresponding gene parameter array at the given index..

Dynamic Parameters

The DynamicParametersIntegerVectorSpecies performs the same task as described above, but obtains the values from a specified class rather than the parameter file. This class must be a subclass of the DynamicSpeciesParameters class. DynamicSpeciesParameter define methods for global, segment and index gene parameters, each return an Option on the value. The default implementation returns None, allowing subclasses to override only the methods they want to be used in the learning run. For out learning run we used the RulesetParams class, which implemented the minGene(index) and maxGene(index) methods. These methods return the min and max value for a particular gene, with the values being obtained via the agent framework:

0.9

[language=scala]
def getIndexType(index: Int): IndexType = (index 
    case n if n < conditionLength => Condition
    case _ => Action

override def maxGene(index: Int): Option[Int] = getIndexType(index) match 
    case Action => Some(Math.max(MWAction.ACTION_FALSE, MWAction.ACTION_TRUE))
    case Condition => (index 
        case Perception(perception) => perception match 
            case bp : BoolPerception => Some(Math.max(bp.TRUE, bp.FALSE))
            case ip : BytePerception => Some(ip.limit.toInt)
        
        ...
    




The setup method of DynamicParametersIntegerVectorSpecies reads the subclass name (in this case RulesetParams) from the parameter file and instantiates it. The prePrototypeSetup method is then used to write the values (overriding those set by IntegerVectorSpecies). It calls each of DynamicSpeciesParameters' methods, if None is returned no action is taken, but if a value is return it is written to the parameter arrays.

0.9

[language=scala]
override def prePrototypeSetup(state: EvolutionState, base: Parameter, default: Parameter): Unit = 
    if (dynamicParamsClassOpt.isDefined) 
        val dynamicParamsClass: DynamicSpeciesParameters
            = dynamicParamsClassOpt.get
        if (dynamicParamsClass.minGene.isDefined) 
            fill(minGene, dynamicParamsClass.minGene.get)
        
        ...
        for(i <- 0 until genomeSize) 
            if (dynamicParamsClass.maxGene(i).isDefined) 
                maxGene(i) = dynamicParamsClass.maxGene(i).get
            
            ...
        
    
    super.prePrototypeSetup(state, base, default)





Ruleset Species

The prePrototypeSetup method of RulesetSpecies checks that the DynamicSpeciesParameter class used is RulesetParams, which contains several utility functions. Adding to globally, by segment and by index methods for declaring gene parameters, this setup also looks for Parameter prefixes `condition' and `action'. If found, it uses a utility function to run loadParametersForGene on all indexes corresponding to the prefix. This method is also passed the prefix, in this way IntegerVectorSpecies specific parameters such as min and max can be set on a condition or action basis without needing to implement them directly. For example, take the following line in the parameter file:

0.9


...species.condition.mutation-prob = 0.1



The `condition' prefix is found by RulesetSpecies setup method, which indirectly loops through all indexes x that pertain to conditions:

0.9

[language=scala]
override def prePrototypeSetup(state: EvolutionState, base: Parameter, default: Parameter): Unit = 
    super.prePrototypeSetup(state, base, default)
    ...
    if(state.parameters.exists(base.push(RulesetSpecies.P_CONDITION), default.push(RulesetSpecies.P_CONDITION))) 
        dpc.runOnIndexes(Condition, genomeSize) 
            (x: Int, mod: Int) => 
                loadParametersForGene(state, x, 
                  base.push(RulesetSpecies.P_CONDITION), 
                  default.push(RulesetSpecies.P_CONDITION), "")
            
        
    




It passes the condition prefix and the index to the loadParameterForGene method. In IntegerVectorSpecies, the suffix `mutation-probability' is added to the condition prefix and the value 0.1 is found, and set for all indexes x in the mutation probability array.



RulesetSpecies also extends the loadParametersForGene method to read in the values for favour mutation from the parameter file. This is controlled by three arrays: favourMutation, a boolean array which holds whether or not a gene should be favour mutated; favourByte, a byte array holding the favoured byte; and favourProbability, the probability with which this byte will be chosen. By extending loadParametersForGene these parameters can be read in on condition or action bases, as well as by segment or individual index.


Mutation

To implement the mutation strategy, the RulesetMutationPipeline was created. It extends the BreedingPipeline and overrides the produce method. It makes no use of the setup method as all mutation parameters are stored in the Species class. The breeding pipeline is threaded, and as such RulesetMutationPipeline is prototypes and cloned for each breed thread. The produce method is called to mutate batches of Individuals, which vary in size.

RulesetMutationPipelines produce method first calls produce on its source (in this case the ESSelection class), which fills the array of individuals to be mutated. It verifies that these individuals are ByteVectorIndividuals and their Species is RulesetSpecies. It then clones each individual and resets its fitness. Lastly it loops through each individual and passes it to the mutateIndividual method.

The mutateIndividual method loops through the individual's genome by index. It uses this index to acquire each gene's corresponding parameters from the species class (e.g. minimum, maximum, mutation probability, favour mutated, favour byte etc.). Using the thread's random number generator stored in the EvolutionState object is decides whether to mutate by calling nextBoolean(mutationProbability). If so it mutates the gene by favour or reset mutation (again decided by the gene parameters). For reset mutation it replaces the gene with a random byte between the gene's minimum and maximum. For favour mutation it makes another call to nextBoolean, with the favour probability parameter. If this returns true, the gene is replaced by the favour byte, otherwise a byte between the minimum and maximum is chosen, excluding the favour byte. The code for this method is given below (edited for brevity).

0.9

[language=scala]
protected def mutateIndividual(state: EvolutionState, thread: Int, vecInd: ByteVectorIndividual, species: RulesetSpecies): ByteVectorIndividual = 
    for (n <- 0 until vecInd.genome.length) 
        if (state.random(thread).nextBoolean(species.mutationProbability(n))) 
            if (species.favourMutation(n)) 
                if (random.nextBoolean(favourProbability))
                    vecInd.genome(n) = species.favourByte(n)
                else
                    vecInd.genome(n) = getRandomByte(
                       species.minGene(n).toByte,
                       species.maxGene(n).toByte,
                       state.random(thread),
                       species.favourByte(n)
                     )
             else 
                vecInd.genome(n) = getRandomByte(
                  species.minGene(n).toByte,
                  species.maxGene(n).toByte,
                  state.random(thread)
                )    
            
        
    
    vecInd.evaluated = false
    vecInd




Evaluation

The Evaluator class prototypes the AgentRulesetEvaluator class and holds one clone for each evaluation thread. The AgentRulesetEvaluator overrides three methods from the Problem class: setup which is called before creating the prototype, prepareToEvaluate which is called once per thread clone before evaluation, and evaluate which is called on single individuals multiple times per thread.

The setup method loads in the MWLevelOptions, MWEvaluationMultipliers, update lambda, number of levels, generational level seeds and the default ruleset action using the EvaluationParamUtil class. As these are built into the prototype they are shared across all evaluation threads, taking advantage of their immutability (explained in Section ). 

The MWEvaluationTask instance is built with these options once per thread, in the prepareToEvaluate method. As the game loop is mutating process it cannot be shared across threads, but as the task can be reset and new agents injecting in in can be used multiple times in one thread.

The evaluate method is passed an individual, which is verified to be a ByteVectorIndividual. The individual's genome is built into a ruleset, which is used to initialise an MWRulesetAgent. This agent, along with the level seed for the current generation, is injecting into the task. The task then evaluates the agent, returning a score, which is attached to the individual as its fitness.


0.9

[language=scala]
override def evaluate(state: EvolutionState, individual: Individual, subpop: Int, thread: Int): Unit = 
    individual match 
        case ind: ByteVectorIndividual => 
            if (task.isDefined) 
                val evalTask = task.get
                val name = this.buildIndAgentName(state, individual, subpop, thread)
                val ruleset: Ruleset = Ruleset.buildFromArray(ind.genome, defaultAction)
                val agent: Agent = MWRulesetAgent(name, ruleset)
            
                val iFitness = evalTask.withAgent(agent)
                                      .withLevelSeed(_taskSeeds(state.generation))
                                      .evaluate



0.9

[language=scala]
                ind.fitness match 
                    case _: SimpleFitness =>  
                        ind.fitness.asInstanceOf[SimpleFitness].setFitness(state, iFitness.toDouble, false)
                        ind.evaluated = true
                    



0.9

[language=scala]
                    case _ => 
                         state.output.fatal("This evaluator (EvolvedAgentRulesetEvaluator) requires a individuals to have SimpleFitness")
                    
                
             else 
                state.output.fatal("Task was not defined when evaluating individual, implying prepareToEvaluate was not run on this instance.")
            
        
        case _ => 
          state.output.fatal("This evaluator (AgentRulesetEvaluator) requires a ByteVectorIndividual")
        
    





Statistics

The Statistics class and its subclasses are setup at the beginning of the learning run and then called as `hook' at various points during the generational loop. RulesetEvolveStatistics implements setup and two of the hooks, postEvaluationStatistics and finalStatistics. The setup method reads the filenames for the generation log, final log and the final agent exports. It registers the log file with EvolutionState's Output instance to allow it to be called from the other methods. 

The postEvaluationStatistics method is passed the EvolutionState, from which it can access the current population and generation number. It loops through the individuals of the current population, calculating the average fitness and determining the best individual. The average fitness, best fitness, current level seed, generation number and the best individuals genome are all written to the generation log using the Output class:

0.9

[language=scala]
val all = " all  " + genNum + "," + levelSeed + "," + avScore + "," + bestScore
state.output.println(
  "--------- GENERATION " + genNum + " ---------", genLog)
state.output.println(all + 
        "Seed    : " + levelSeed + 
        "Score : " + avScore + 
        "Score    : " + bestScore + 
        "Agent    :-" +
        "" + agentStr + 
        "genLog)


 
The best individual in the generation is saved to the currentBestIndividual field. Further checks to see if it is the best overall individual or has the biggest difference to the average are also performed, resulting in the individual being saved to the bestOverallIndividual and biggestDiffIndividual fields respectively. These checks are also controlled by generation number, only being performed after a certain generation, specified in the parameter file.

The finalStatistics method retrieves the currentBestIndividual, overallBestIndividual and biggestDiffIndividual. From each it constructs a ruleset, initialises an agent with the correct default action and passes it to agent IO utility class to be persisted into the corresponding filenames captured during setup. The entire final generation and their fitness values are also written to the final log.






Testing


ECJ's structure makes unit testing extremely difficult, due mainly to its heavy reliance on the EvolutionState singleton. It is utilised by every class and passed to nearly every method. ScalaMock is unable to mock this object as it is not accessed through an interface and building a fixed instance would be overly time consuming (and tantamount to simply running the software).

Ultimately, besides the EvaluationParamsUtil class, no testing was performed. Instead, the implementation adopted a `fail loudly' approach, where any discrepancy was logged and triggered a program shutdown. In this way errors were easily noticed and corrected.

The lack of testing is not desirable, and given more time, efforts would have been made to rectify this. However, as the software has no external `users' (it being used as a tool solely by those that wrote it) it did not affect the final result.






Running


As learning could take several hours, or even days, the software was run on an external quad-core server. Maven was used to package the main project, with all its dependencies in to a jar file, which allowed it to easily transferred and run. As the project went through several revisions and performed many learning runs, shell scripts were written to facilitate this process. The push script called Maven to package the project, running all tests, which was then copied to the server (using ssh), along with parameter files and their runner scripts. These runner scripts were then run remotely to begin the learning process. On completion another script retrieved the generation log and the agent files.

Immutability and thread safety were a major focus of the project. This was done to allow the evaluation and breeding stages to be run over four threads (to take advantage of the server's quad-core). However, during the initial run it was quickly discovered that multithreading the evaluation process was impossible. Even though each thread had its own evaluation task instance, many of the game engine's assets were held statically and were mutable. Moreover, the MarioEnvironment was implemented as a singleton. This meant only one task could be performed at a time, resulting in the evaluation stage being set to run in only one thread. Despite this setback, learning runs took approximately 6 hours to complete.





Parameters and Revisions




























































Many learning runs were performed during the project, including numerous parameter revisions. For brevity, only the most significants changes will be presented.

Initial runs used parameter files that closely followed the approach used by the REALM agent team. Conditions favoured the DONTCARE byte with a probability of , whereas actions were reset mutated. Every gene had a mutation probability of 0.2. Agents were evaluated over 10 levels in a similar style to REALM's method. The levels varied over a wide range of difficulties, but favoured easier levels. Enemies were enabled for all but 2 levels. The evaluation multipliers rewarded distance, level completion, Mario's final mode, kills and time left upon completion.

The agents produced with these parameters did not display any interesting behaviours and functioned similarly to the handcrafted Forward Jumping agent (). the first revision aimed to address this.

Mutation probability of both conditions and actions was lowered and the DONTCARE condition favour probability was increased to 0.5. The biggest change came with the evaluation task. Agents now played a narrower band of difficulties, generally more difficult than the first set. Enemies were only enabled for the last 4 levels and pits were enabled for all. These changes were made as they suited a reactive agent over the blind forward jumping approach. Lastly, the level completion was given more weight.

Agents evolved with these parameter demonstrated much more compelling behaviours, as well as higher overall attainment. However, it was observed that much of their ruleset's were unused.

A second revision was made with an aim to increasing the number of rules used and to reduce the search space. Favour probability for genes corresponding to conditions on MarioMode and EnemyLeft were greatly increased, making DONTCARE likely in these conditions on all rules. For the clarity, the learning run performed using these parameters will be referred to as the LEMMEL run (for LEss MarioMode and EnemyLeft)

The agents produced with this approach are evaluated in depth on Sections  and . Tables ,  and  summarise the LEMMEL parameters, which can be found in full, in ECJ's file format, in Appendix .
















Results





Learning Data



 









Statistics output of learning module collated and organised.
Figure  shows the data in graphical form. It includes the best and average fitness of each generation, with polynomial trend line.
Fitness of the handcrafted agent (on playing same levels) also included for comparison.
Agents evolved during this run are analysed in more detail in Section .


Improvement over Time


Graph shows improvement in both average and best fitness.
During early generation improvement is rapid, but slows considerably after 200 generations.
Mainly due to agents saturating the evaluation task; 70,000 is a very high score, marking the completion of all levels. Anything over 60,000 is very capable agent (in terms of evaluation task).
We also see form the trend line that the best fitness is often better than the handcrafted complex agents fitness.




Generation Variance













It is evident from the graph that there is a lot of variance in both the best and average fitness between generations.
(Will include measurements of variance)
This could be explained by the rule based agent framework being sensitive to mutation, or the level playing module level seed affecting difficulty (explained further in Section )
Due to this variance a learning run was performed that had a fixed level seed, so that each generation plays exactly the same levels, the graph is included as Figure .
As expected (and due to the nature of  ES) this removes all variance in the best fitness, but some still remains in average fitness.
Also the maximum fitness achieved by an agent in the run was higher.
(Graph also shows an issue with benchmark; non-determinism. The fitness jumps and falls at generation 882, this fitness calculation for the best agent was different between gen 882 and gen 883, and is unrepeatable in manual tests.)
The final agent produced from the fix seed run is included in the agent analysis below.


Population Variance


Both graphs show a large difference between the best and average fitness. This suggest a large variance in fitness over a single population.
The last generation of the fix seed run has a standard deviation ...
As the ES ensures each generation's population are mutated from the previous best agents, this shows that the agent framework is particularly sensitive to mutation, which ruleset fitness dropping dramatically in one mutation round.
For example the final agent of the fixed seed run has a rule as such: ---- The worst agent of the final generation mutated (with only one mutation) this rule to: ---- Which instructed the agent to never move, causing a drop in score from 74xxx to 4xxx.







Handcrafted vs. Learnt Agent



Three learnt agents were taken from the learning runs to be analysed here: the Learnt Difference agent, which performed best compared to the average fitness (past generation 800); the Learnt Best agent, which got the highest fitness overall (past generation 800) and the Learnt Fixed Seed agent, which was the final agent from the fixed seed run.
This section will compare them with the handcrafted agents, in both the learning evaluation task and the comparator task (Section ).


Learning Evaluation Task





















Three chosen learnt agents and three handcrafted agents were run through the each generation's evaluation task of the final learning task (i.e. 1000 tasks, with the same options, different seeds). Results are compiled in to Table 
We can see from the data that both the Leant best and Learnt difference agents performed at the same level as the complex agent and considerable better than the other handcrafted agents. Which implies they became very capable at the task they were learning on.
We can also see that the learnt fixed seed agent did not perform quite as well, suggesting it became too suited to the single set of levels it was evolved over.
The large difference between best and worst fitness for each agent reiterates the problem of variance may stem from the level playing modules use of level seed, which may be overly affecting difficulty.



Comparator Task























 









Three chosen learnt agents and three handcrafted agents were run through the comparator task with seed of 1000. Results are compiled in to Table 
We can see from the data that both the Leant best and Learnt difference agents performed better than the two simpler handcrafted agents, but not as well as the complex agent. Considering the evaluation task result, this shows that the learning evaluation task it doesn't represent the comparator task well enough
We can also see that the learnt fixed seed agent did not perform quite as well, suggesting learning from many sets of levels helps when tackling different challenges.
Figure  shows how the agents performed at different difficulties in the comparator task. Learnt agents were outperformed by complex agent on all difficulties (especially 6, 5 and 4) except 2, which was the most prevalent in the learning evaluation task. Implying that a longer and more representative task may produce a better agent.






Learnt Agent Analysis








































Of the learnt agents the learnt difference agent performs the best in the comparator task
It also displays the most interesting behaviour which will be discussed here
A full ruleset for this agent is included in Figure  


Rule usage


The agent only regularly uses 7 of its 20 rules, and relies heavily on the default action.
Suggests mutation strategy might be too simplistic for ruleset genome.


Behaviours


The agent acts similarly to the complex and simple reactive agent. It jumps over pits, obstacles and enemies.
Four behaviours are presented in depth below. 


Obstacles







































Most interesting, seen on most learnt agents.
Evolved early, generation 50 or so.
Uses rules 5 and 9 to jump over obstacles. But only when Mario is stationary.
This is very useful for stairs (obstacles before pits) as the agent avoids jumping into the pit, something common with the simple reactive agent.
Figure  shows this process. Agent runs until he hits the wall, activates rule 5 to jump, rule 9 keeps the jump until top height, when coming down uses default rule to move right.
This is an example of a behaviour not considered in handcrafting, as the complex agent solves this problem differently.



Pits








































Similar but not equivalent system seen on most learnt agents.
Uses rules 16, 2, 8, 13 to jump over pits and land on the very edge of the other side.
This is useful as Mario avoids falling in a proceeding pit or touching an enemy.
Figure  shows this process. Agent uses obstacle behaviour to climb the stairs. Then uses rule 16 to begin the jump, Rule 2 moves the agent forward at speed when moving up, Rule 8 continues this action as he moves down. When over the pit Rule 13 ensures he doesn't go too far by moving him left until he touches the ground.




Enemies








































Similar but not equivalent system seen on most learnt agents.
Uses rules 18, 2, and 13 to jump over enemies and clear the landing zone with a fireball.
Figure  shows this process. Agent approaches an enemy and uses rule 18 to jump, Rule 2 continues the jump and fires a fireball (if in fire mode). This fireball beats Mario to the landing zone, where it can clear enemies.




Pits with Enemies








































An advanced behaviour that is not seen in other agents.
Uses rule 19 to avoiding jumping into enemies when clearing pits.
Figure  shows this process. Agent approaches an pit and if there is an enemy blocking the jump, it turns around and moves away. When the enemy moves or falls agent jumps the pit.

















Evaluation


With the results presented above, we can now critically consider the project in terms of the requirements set out in the project's aim, objectives (Section ) and specification (Section ).





Agent Framework

The primary objective of the agent framework was to enable and encourage meaningful improvement during learning. The focus on a rule based system allowed for agents to be represented with a simple encoding, which afforded a great deal of freedom to the learning module. It reduced development time as several features of the ECJ library could be used as is and any extensions did not have to process a complex data structure. Moreover, it allowed for more greater control, which was crucial in honing and revising the learning parameters. Ultimately, we can see from the results that, given the right parameters, the agent framework did enable significant improvement over generation, with later agents achieving near maximum fitness.

However, the results show a high fitness deviation over each generations population. This is likely due to the inherent sensitivity to mutation of a rule-based approach. One gene mutation can transform a once useful rule into one that is counter productive, with a great affect to the agent's capability.

In addition to the project's aims and objective, three criteria were laid out for the agent's use of a rule based system in Section : speed, capability and complexity. The framework design relies heavily on how the agent senses the environment; as such the selection of perceptions is a major factor in its evaluation.

The complex handcrafted ruleset shows that the framework's choice of perceptions allows for capable agents. It rarely fails to finish the easier levels and can successfully tackle some of the more difficult challenges. Furthermore, the depth of the perceptions and the available actions allows for competent behaviour to be demonstrated by relatively small rulesets, which improves the agent's speed. In fact, agent response time never rises above a millisecond. This allows for levels to be run at practically any number of frames per second and has little effect on total learning time.

On the other hand, the framework is still limited. There is no way for any ruleset agent to break itself out of a loop or navigate a dead end. Similarly, it can never develop an effective enemy killing behaviour without differentiate between different types.

However, as discussed in Section , improving capability by embellishing the agent's perceptions comes at the penalty of complexity. A more complex design lends to a larger search space for the learning algorithm, which hinders the development of a successful agent through random mutation. This was also a crucial consideration of the REALM team, whose V2 agent was designed, in part, to reduce the search space of the VI agent [p. 86].

In its current form, the agent framework balances these factors reasonably well. Our results show that a successful and interesting agent can be evolved despite the total number of possible rules



being higher than that of REALM's V1 agent () [p. 86]. Thus, improvements to the capability of the agent should not increase the search space, but instead focus on making each perception is purposeful.

In the LEMMEL run parameters (and in the handcrafted agents) we encouraged the disregarding of the MarioMode and EnemyLeft perceptions. Our learning parameters allowed us to reduce the search space by increasing the probability of the DONTCARE condition. This is clearly a counter intuitive approach and the agent design should be amended by removing these perceptions. 

Such removals would allow for additional perceptions and/or design extensions without increasing agent complexity. Given more time, a redesign that allowed agent encoding control over its perceptions would be considered. Perceptions would remain fixed, but the parameters that control them would be variable. For example, the area in which an agent looks for an obstacle or enemy could be controlled by parameters on a ruleset wide basis. Moreover, perceptions that ascertain the last used action (a simplistic form of memory) and greater detail in the identifying enemies would also be investigated.






Level Playing

The primary motivation for the level playing module was to meet Objective : Provide a large and diverse test bed from which to learn. This was reiterated in the project specification (Section ), which decrees that it must, in a parametrised manner, produce a variety of levels and ways to score an agent.

The former was achieved by providing access to the benchmark's level generation options through simple data classes. Additionally, as detailed in Section , modifications were made to the LevelGeneration class to provide greater diversity and to ensure parameters were being treated correctly. This was an unexpected workload, and as such could not be afforded ample attention. Without time constraints, further modifications would have been, including improved scaling of enemy types and better block placement. Furthermore, these and previous changes, as well as other level generation features, would have been made parametrisable, allowing the MWLevelOptions class more power and precision.

The latter was achieved by the MWEvaluationMultipliers class, which parametrised a linear function over level playing statistics. The options mirrored the statistics produced by the benchmark software. Whilst this was effective in calculating agent fitness in the learning module, additional statistics would be beneficial. For example, it is currently impossible to punish an agent for the manner in which it failed (i.e. by falling into a pit, by colliding with an enemy, or by running out of time). If the project was to continue, the reliance of the benchmark software for these statistics would be reduced and a specialised EvaluationInfo class would be implemented, allowing for the level playing statistics to be extended.

Specifying and persisting these two elements was accomplished by extending ECJ's parameter file system. Adopting this existing system saved time and ensured it integrated well with the learning module. With the high number of revisions and the emergent importance of the evaluation task during the project, this decision proved vital. Whilst it provided great control over a small number of levels, it was too verbose to be used for longer level tasks. For example, the comparator task (Section  and Appendix ) had to be hard-coded rather than specified in a level options file. With more time, the vocabulary of the parameter persistence would be extended to include controls for segments of episodes and values as a function of episode number (as seen in the comparator task).

We saw in the results that the learning runs featured a high level of inter-generational variance. This is to be expected, especially of the average fitness, due partly to the agent framework's sensitivity to mutation and partly to the nature of Evolutionary Strategies. However, the variance is also displayed by the handcrafted agent, which suggests that variance may in fact be caused by the difficulty of the evaluation task fluctuating between generations. As the only task parameter that changes between generations, the choice of level seed may be having too much of an effect on level generation. Ways to tackle this issue within the parameters of the learning algorithm are discussed in the next subsection, however, if given additional time the influence of the level seed on level difficulty would be investigated.

Aside from level generation, the benchmark software also raised several other barriers to the project. Firstly, there was little to no documentation, which slowed the initial progress of the project. Secondly, it greatly hindered the testability of the project as a whole (as discussed in Section ), making it very difficult to provide significant test coverage (as prescribed by Objective ). Lastly, it impeded the project's ability to meet the non-functional requirement on thread safety. As mentioned in Section , the game-engine is implemented as a singleton, relying heavily on static assets. A continued project would look to refactor this and allow the evaluation stage of the learning process to be multithreaded.







Learning Process

Section  described the revision of the learning parameters during the project. The results show that honing these parameters was crucial to the success of the project. Due to time constraints parameter adjustment had to be stopped to allow for results to be gathered and analysed. Without such a restriction more parameter revisions would have been made.

The first would have continued the attempt to reduce generational variance (in conjunction with solutions mentioned in the previous two sections). We saw that having a single level seed for the entire run removed variance from the best fitness (as expected) and lowered variance in the average fitness. However, it produced a less capable agent, which was overly suited to the small number of levels it played (known as overfitting). Instead a compromise would be struck, perhaps taking seeds from a fixed pool, or have them run in generational segments. In this way agents are evaluated over a wider range of levels, but not at a hinderance to the learning evolutionary process.

The second would investigate reducing overfitting in general. We saw in the results that the learnt agent performed better than the complex handcrafted agent at the learning evaluation task, but did not perform as well on the comparator task. We also saw that in both the fixed and variable seed run that the learnt agent achieved near perfect score relatively early, suggesting the task could be made more difficult. In future revisions the learning task would be run on higher difficulties, with more focus on enemies. Also, with the ability to multithread the evaluation, a more extensive task could be run, without a time penalty. Increase the number of levels would allow the task to be more representative of the comparator task.



Even without the above revisions we have seen that the use of an Evolutionary Strategy has proved successful. It has allowed for our aim to be achievement, as well as meeting the project specification. Meaningful learning was observed and the final agent demonstrated interesting behaviours, including some that were unforeseen and not considered during the creation of handcrafted rulesets. Moreover, as the use of truncation selection is known to reduce variance in learning algorithms  and due to the variance induced by both the agent framework's mutation sensitivity and the level playing's seed variation, the choice of an evolution strategy turned out to be very appropriate.

However, because of the scope of the project, its implementation is quite simplistic. With more time several additional features would be considered. First would be the use of rule crossover, as seen in the REALM team's approach (described in Section ).Second would be utilising a Tabu (taboo) list, an common approach in optimisation search algorithms . Rules that caused a significant drop in fitness would be added to a blacklist and would be avoided by future mutations. Last would be adaptive mutation rates, a common feature of ESes . The results showed that the learn agent only used a small percentage of their ruleset, to combat this mutation rates could change during a learning run on a rule by rule basis. If a rule goes unused in the evaluation stage, then its mutation rate is increased, if it is used often, then its mutation rate is decreased.

The result that were gathered from the statistical output of the learning module have proved a good analysis of the learnt agent and the process in general. However, it was impossible to properly ascertain key metrics such as population variance and mutation sensitivity. With more time, the fitness and genome of each individual in the learning run would be made available, allowing for a more in depth analysis.






Methodology

Overall the project completed the majority of its objectives and enacted most of the project specification. Its failure to attain all of its aims was centred around two aspects: testing and testability and issues with the Mario benchmark software.

Testing proved more difficult and time consuming than anticipated, due in part to the inclusion of ECJ and the benchmark (see Sections ,  and ). For several parts of the level playing and learning modules testing was infeasible. This precipitated the abandoning of test driven development (Objective ) and reduced the test coverage (Objective ) of the entire project.If the project were to be attempted again, more time and effort would be spend to make the code more testable.

The Mario benchmark software came with several problems (see Section ). Those that were fixed, were time-consuming and caused a schedule overrun. Those that could not be addressed lead to not being able to fully adhere to the project specification (e.g. multithreading the learning process as discussed in Section ). More research should have gone in the source code at an earlier date, which would have helped identified these issues sooner, allowing preparations to be made.

Due to the modifications of the benchmark software, comparisons to other approaches were limited. Framing the learnt agent in terms of the Mario AI Competition would have been misleading and uninformative as it used an engine that produced much more difficult levels. However, greater effort should have gone toward making these comparisons, especially to the REALM agents. Given more time, the learnt agent would have performed tests in the original engine, allowing score comparisons to the REALM agent and other competition entrants.

Despite these issues, the project met its overarching objective. It successfully created a interesting and capable game-playing agent, developed by evolutionary computation.


















toctocdepth2

toc
  
  
  











0

0

0




Perceptions


-1





















0.9
[language=scala, basicstyle=, caption=Pit detection in the perceptions classes., label=lst:prcptpit]
// Returns NONE if there are no pits ahead, FAR and CLOSE if there is one in a certain number of columns relative to mario.
case object PitAhead extends BytePerception(7, 2) 
    val NONE: Byte = 0; val CLOSE: Byte = 1; val FAR: Byte = 2;
    val COL_CLOSE_L = 1; val COL_CLOSE_R = 1; val COL_FAR_L = 2; val COL_FAR_R = 2
    def apply(environment: Environment): Byte = 
        val pitOp: Option[Int] = 
          Perception.getOpens(environment, COL_CLOSE_L, COL_FAR_R).headOption
        pitOp match 
            case Some(x) if (x >= COL_CLOSE_L && x <= COL_CLOSE_R) => CLOSE
            case Some(x) if (x >= COL_FAR_L && x <= COL_FAR_R) => FAR
            case _ => NONE
       
    


def getOpens(environment: Environment, a: Int, b: Int): List[Int] = 
    val level = environment.getLevelSceneObservationZ(2);
    val test = (x: Byte) => x == 0  x == GeneralizerLevelScene.COIN_ANIM;
    val bottomRow = level.length - 1
    val mario = getMarioPos(environment)
    val left = max(0, min(a, b) + mario._2)
    val right = min(level(0).length, max(a, b) + mario._2)   
    var opens: List[Int] = left to right toList
    
    for 
        i <- mario._1 + 1 to bottomRow
        if (!opens.isEmpty)
    
        opens = opens.filter  j => level(i)(j) == 0  level(i)(j) == GeneralizerLevelScene.COIN_ANIM 
    
    opens.map  x => x - mario._2 




Handcrafted Agent Rulesets

0

Included below are the full rulesets for the project's handcrafted agents. Blank entries denote a DONTCARE condition or a FALSE action.









































































Full Level and Evaluation Options

0

0.9
[language=scala, basicstyle=, caption=Full field definitions for MWLevelOptions and MWEvaluationMultipliers described in Section ]
/***
 * Options that control level generation
 */
class MWLevelOptions(
    val blocks: Boolean,   // Blocks appear
    val cannons: Boolean,  // Cannons appear
    val coins: Boolean,    // Coins appear
    val deadEnds: Boolean, // Dead ends appear in terrain forcing Mario to turn back
    val enemies: Boolean,  // Enemies/Creatures appear
    val flatLevel: Boolean, // Level is flat, no change in elevation
    val frozenCreatures: Boolean, // All creatures don't move
    val pits: Boolean,     // Pits appear
    val hiddenBlocks: Boolean, // Hidden blocks appear
    val tubes: Boolean,    // Tubes/Pipes appear
    val ladders: Boolean,  // Ladders appear
    val levelDifficulty: Int, // Difficulty of level, effective range 0-25, 0 easiest
    val levelLength: Int,  // Length of level in blocks
    val levelType: Int,    // Type of level, 0-Outside 1-Cave, 2-Castle
    val startingMarioMode: Int, // Mode Mario starts as 0-small, 1-big, 2-fire
    val timeLimit: Int     // Number of Mario seconds allowed to complete level
)

/***
 * Multipliers for several level playing statistics.
 * Comments describe the statistic. For example, if Mario
 * completes the level It will be win * 1, otherwise it will win * 0
 */
class MWEvaluationMultipliers(
    val distance: Int, // Distance travelled by Mario in pixels (16 pixels to a block)
    val win: Int,      // 1 for level complete, 0 otherwise
    val mode: Int,     // Mario's final mode on completion or death, 2-fire, 1-big, 0-small
    val coins: Int,    // Number of coins collected
    val flowerFire: Int, // Number fire flowers collected
    val kills: Int,    // Number of enemy kills
    val killedByFire: Int, // Number of kills by fireball
    val killedByShell: Int, // Number of kills by shell
    val killedByStomp: Int, // Number of kills by stomp
    val mushroom: Int, // Number of mushrooms collected
    val timeLeft: Int, // Mario seconds left on completion, 0 if level not completed
    val hiddenBlock: Int, // Number of hidden blocks hit
    val greenMushroom: Int, // Number of green mushrooms collected
    val stomp: Int   // Unused
)



Comparator Task Options

0


0.9
[language=scala, basicstyle=, caption=Parameter classes for the comparator task described in Section ]
val defaultEvaluationMultipliers = new MWEvaluationMultipliers(
        	1,    //Distance
        	2048, //Win
		16,   //Mode
		16,   //Coins
		64,   //FlowerFire
		58,   //Mushroom
		42,   //Kills
		4,    //KilledByFire
		17,   //KilledByShell
		12,   //KilledByStomp
		8,    //TimeLeft
		24,   //HiddenBlock
		58,   //GreenMushroom
		10)   //Stomp
		
val compBaseOptions: MWLevelOptions = new MWLevelOptions(
		true,  //blocks
		true,  //cannons
		true,  //coins
		false, //deadEnds
		true,  //enemies
		false, //flatLevel
		false, //frozenCreatures
		true,  //gaps
		false, //hiddenBlocks
		false, //tubes
		false, //ladders
		0,     //levelDifficulty
		256,   //levelLength
		0,     //levelType
		2,     //startingMarioMode
		200)   //timeLimit

def compUpdate(levelSeed: Int): (Int, MWLevelOptions) => MWLevelOptions = (i: Int, options: MWLevelOptions) => 
    options.withLevelLength(
             ((((i+levelSeed) * 431) 
           .withTimeLimit((options.levelLength * 0.7).toInt)
           .withLevelType(i 
           .withLevelDifficulty((compNumberOfLevels - i)/32)
           .withPits(i 
           .withCannons(i 
           .withTubes(i 
           .withCoins(i 
           .withBlocks(i 
           .withLadders(i 
           .withFrozenCreatures(i 
           .withEnemies(!(i 
           .withStartingMarioMode(
               if (i 
                 if (i 
                else 2)





LEMMEL Learning Parameter File


0.9

parent.0 = @ec.es.ESDefaults es.params

#General
breedthreads 	= 4
evalthreads 	= 1
seed.0 = 1
seed.1 = 909
seed.2 = 499311
seed.3 = 90032

# +++ ES +++
breed = ec.es.MuPlusLambdaBreeder
es.mu.0 		= 5
es.lambda.0 	= 45
generations 	= 1000

# +++ POP +++
pop.subpops 						= 1
pop.subpop.0.size 					= 50
pop.subpop.0.species 				= com.montywest.marioai.learning.ec.vector.RulesetSpecies
pop.subpop.0.species.fitness		= ec.simple.SimpleFitness

#	 Rulelength 15, So 20 rules gives 300
pop.subpop.0.species.genome-size 	= 300
pop.subpop.0.species.ind 			= ec.vector.ByteVectorIndividual

#	These will be ignored, but warnings otherwise 
pop.subpop.0.species.min-gene		= -1
pop.subpop.0.species.max-gene		= 2
pop.subpop.0.species.mutation-type	= reset
pop.subpop.0.species.mutation-prob  = 0.0
pop.subpop.0.species.crossover-type = one

#	Handles min and max
pop.subpop.0.species.dynamic-param-class = com.montywest.marioai.learning.ec.params.RulesetParams

#	Condition params
pop.subpop.0.species.condition							= true
pop.subpop.0.species.condition.mutation-prob 			= 0.05
pop.subpop.0.species.condition.favour_byte 				= -1
pop.subpop.0.species.condition.favour_probability 		= 0.5

# MarioMode Condition
pop.subpop.0.species.condition.0.favour_probability 	= 0.95

# EnemyLeft Condition
pop.subpop.0.species.condition.3.favour_probability 	= 0.9



0.9

#	Action params
pop.subpop.0.species.action								= true
pop.subpop.0.species.action.mutation-prob 				= 0.09


# +++ STATS +++
stat.num-children 						= 1
stat.child.0 							= com.montywest.marioai.learning.ec.stats.RulesetEvolveStatistics
stat.child.0.gen-file					= ../lmm-lel-gen.stat
stat.child.0.final-file 				= ../lmm-lel-final.stat
stat.child.0.final-agent-file			= lmm-lel-final
stat.child.0.best-agent-file			= lmm-lel-best
stat.child.0.diff-agent-file			= lmm-lel-diff
stat.child.0.best-agent-limit			= 800
stat.child.0.diff-agent-limit			= 800


# +++ MUTATION +++
pop.subpop.0.species.pipe.source.0 		= ec.es.ESSelection
pop.subpop.0.species.pipe 				= com.montywest.marioai.learning.ec.vector.breed.RulesetMutationPipeline


# +++ EVAL +++
eval.problem							= com.montywest.marioai.learning.ec.eval.AgentRulesetEvaluator

#	Seeds for generating levels
#	each generation g, seed used is:
#		prev_seed + add + g*mult
#   where prev_seed is seed_start on g  = 0
eval.problem.seed						= true
eval.problem.seed.start					= 3348
eval.problem.seed.add					= 284839
eval.problem.seed.mult					= 2568849

#	Evaluation Multiplier
eval.problem.mults						= true
eval.problem.mults.distance				= 1
eval.problem.mults.win					= 3200
eval.problem.mults.mode					= 200
eval.problem.mults.kills				= 100
eval.problem.mults.time-left			= 2

#	Fallback Action
eval.problem.fallback-action			= true
eval.problem.fallback-action.right		= true
eval.problem.fallback-action.speed		= true



0.9

#	Levels
eval.problem.level						= true
eval.problem.level.num-levels			= 10

eval.problem.level.base.dead-ends 		= false
eval.problem.level.base.enemies			= false
eval.problem.level.base.cannons			= false
eval.problem.level.base.pipes			= false
eval.problem.level.base.start-mode-num  = 2
eval.problem.level.base.length-num		= 200
eval.problem.level.base.time-limit		= 100
eval.problem.level.0.difficulty-num		= 2
eval.problem.level.0.type-num			= 1
eval.problem.level.1.difficulty-num		= 3
eval.problem.level.1.type-num			= 0
eval.problem.level.2.difficulty-num		= 5
eval.problem.level.2.type-num			= 2
eval.problem.level.3.difficulty-num		= 10
eval.problem.level.3.type-num			= 0
eval.problem.level.4.difficulty-num		= 2
eval.problem.level.4.flat				= true
eval.problem.level.5.difficulty-num		= 7
eval.problem.level.6.difficulty-num		= 2
eval.problem.level.6.flat				= false
eval.problem.level.6.enemies			= true
eval.problem.level.6.frozen-enemies		= true
eval.problem.level.7.difficulty-num		= 2
eval.problem.level.7.frozen-enemies		= false
eval.problem.level.8.difficulty-num		= 3
eval.problem.level.8.type-num			= 1
eval.problem.level.8.tubes				= true
eval.problem.level.9.difficulty-num		= 5
eval.problem.level.9.type-num			= 0
eval.problem.level.9.cannons			= true
eval.problem.level.9.blocks				= false
eval.problem.level.9.tubes				= false






Full Source Code

Agent Framework

Mario AI Benchmark Modifications

Level Playing Module

ECJ Modifications

Learning Module

Program Entry Point


toc

















11


  Siyuan Xu,
  History of AI design in video games and its development in RTS games,
  Department of Interactive Media  Game development, Worcester Polytechnic Institute, USA,
  https://sites.google.com/site/myangelcafe/articles/history_ai.


  Chad Birch,
  Understanding Pac-Man Ghost Behaviour,
  http://gameinternals.com/post/2072558330/understanding-pac-man-ghost-behavior,
  2010.
  

  Alex J. Champandard,
  The AI From Half-Like's SDK in Retrospective,
  http://aigamedev.com/open/article/halflife-sdk/,
  2008.
  

  Tommy Thompson,
  Facing Your Fear,
  http://t2thompson.com/2014/03/02/facing-your-fear/,
  2014.


  Damian Isla,
  GDC 2005 Proceeding: Handling Complexity in the Halo 2 AI,
  http://www.gamasutra.com/view/feature/130663/gdc_2005_proceeding_handling_.php,
  2005.
  

  Alex J. Champandard,
 Monte-Carlo Tree Search in TOTAL WAR: Rome II Campaign AI,
 http://aigamedev.com/open/coverage/mcts-rome-ii/,
 2014.
 

  Georgios N. Yannakakis, Pieter Spronck, Daniele Loiacono and Elisabeth Andre,
  Player Modelling,
  http://yannakakis.net/wp-content/uploads/2013/08/pm_submitted_final.pdf,
  2013.
  

  Matt Bertz,
  The Technology Behind The Elder Scrolls V: Skyrim,
  http://www.gameinformer.com/games/the_elder_scrolls_v_skyrim/b/xbox360/archive/2011/01/17/the-technology-behind-elder-scrolls-v-skyrim.aspx,
  2011.
  

  The Berkeley Overmind Project,
  University of Berkeley, California.
  http://overmind.cs.berkeley.edu/.
  

  Simon M. Lucas,
  Cellz: A simple dynamical game for testing evolutionary algorithms,
  Department of Computer Science, University of Essex, Colchester, Essex, UK,
  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.5068&rep=rep1&type=pdf. 
  

  G. N. Yannakakis and J. Togelius,
  A Panorama of Artificial and Computational Intelligence in Games,
  IEEE Transactions on Computational Intelligence and AI in Games,
  http://yannakakis.net/wp-content/uploads/2014/07/panorama_submitted.pdf,
  2014.
  

  Julian Togelius, Noor Shaker, Sergey Karakovskiy and Georgios N. Yannakakis,
  The Mario AI Championship 2009-2012,
  AI Magazine 34 (3), pp. 89-92,
  http://noorshaker.com/docs/theMarioAI.pdf,
  2013.


  Richard S. Sutton and Andrew G. Barto,
  Reinforcement Learning: An Introduction
  The MIT Press, Cambridge, Massachusetts, London, England,
  Available: http://webdocs.cs.ualberta.ca/ sutton/book/ebook/the-book.html,
  1998.

 
  Gary B. Fogel et al.,
  Evolutionary Programming
  Scholarpedia, 6(4):1818.,
  http://www.scholarpedia.org/article/Evolutionary_programming,
  2011.
  

  Stuart J. Russell, Peter Norvig,
  Artificial Intelligence: A Moden Approach (3rd ed.,
  Upper Saddle River, New Jersey,
  2009.
  

  Stuart J. Russell, Peter Norvig,
  Artificial Intelligence: A Moden Approach (1st ed.,
  Upper Saddle River, New Jersey,
  1995.
  

  Anoop Gupta, Charles Forgy, Allen Newell, and Robert Wedig,
  Parallel Algorithms and Architectures for Rule-Based Systems,
  Carnegie-Mellon University Pittsburgh, Pennsylvania,
  ISCA '86 Proceedings of the 13th annual international symposium on Computer architecture, pp. 28-37,
  1986.
  

  Melanie Mitchell,
  An Introduction to Genetic Algorithms,
  Cambridge, MA: MIT Press,
  1996.


  Hans-Georg Beyer and Hans-Paul Schwefel,
  Evolution Strategies: A Comprehensive Introduction,
  Natural Computing 1: 3?52,
  2002.


  Andrew Y. Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger and Eric Liang,
  Inverted autonomous helicopter flight via reinforcement learning,
  International Symposium on Experimental Robotics,
  http://www.robotics.stanford.edu/ ang/papers/iser04-invertedflight.pdf,
  2004.
  

  S. Singh, D. Litman, M. Kearns and M. Walker,
  Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System,
  Journal of Artificial Intelligence Research (JAIR), Volume 16, pages 105-133, 2002,
  http://web.eecs.umich.edu/ baveja/Papers/RLDSjair.pdf.
  

  Alex J. Champandard,
  Making Designers Obsolete? Evolution in Game Design,
  http://aigamedev.com/open/interview/evolution-in-cityconquest/,
  2012.
  

  James Wexler,
  A look at the smarts behind Lionhead Studio's `Black and White' and where it can and will go in the future,
  University of Rochester, Rochester, NY 14627,
  http://www.cs.rochester.edu/ brown/242/assts/termprojs/games.pdf,
  2002.
  

  Thore Graepal (Ralf Herbrich, Mykel Kockenderfer, David Stern, Phil Trelford),
  Learning to Play: Machine Learning in Games,
  Applied Games Group, Microsoft Research Cambridge,
  http://www.admin.cam.ac.uk/offices/research/documents/local/events/downloads/tm/06_ThoreGraepel.pdf.
  

  Drivatar in Forza Motorsport,
  http://research.microsoft.com/en-us/projects/drivatar/forza.aspx.
  

  Sergey Karakovskiy and Julian Togelius,
  Mario AI Benchmark and Competitions,
  http://julian.togelius.com/Karakovskiy2012The.pdf,
  2012.
  

  Julian Togelius, Sergey Karakovskiy and Robin Baumgarten,
  The 2009 Mario AI Competition,
  http://julian.togelius.com/Togelius2010The.pdf,
  2010.
  

  Julian Togelius,
  How to run a successful game-based AI competition,
  http://julian.togelius.com/Togelius2014How.pdf,
  2014.
  

  TORCS: The Open Racing Car Simulation,
  http://torcs.sourceforge.net/.


  Daniele Loiacono, Pier Luca Lanzi, Julian Togelius, Enrique Onieva, David A. Pelta, Martin V. Butz, Thies D. Lnneker, Luigi Cardamone, Diego Perez, Yago Sez, Mike Preuss, and Jan Quadflieg,
  The 2009 Simulated Car Racing Championship,
  IEEE Transactions on Computational Intelligence and AI in Games, VOL. 2, NO. 2,
  2010.
  

  The 2K BotPrize,
  http://botprize.org/
  

  Michael Buro, David Churchill,
  Real-Time Strategy Game Competitions,
  Association for the Advancement of Artificial Intelligence, AI Magazine, pp. 106-108,
  https://skatgame.net/mburo/ps/aaai-competition-report-2012.pdf,
  2012.
  

  Infinite Mario Bros.,
  Created by Markus Perrson,
  http://www.pcmariogames.com/infinite-mario.php.
  

  H. Handa,
  Dimensionality reduction of scene and enemy information in mario,
  Proceedings of the IEEE Congress on Evolutionary Computation,  
  2011.
  

  S. Ross and J. A. Bagnell, 
  Efficient reductions for imitation learning,
  International Conference on Artificial Intelligence and Statistics (AISTATS),
  2010.
  

  Slawomir Bojarski and Clare Bates Congdon,
  REALM: A Rule-Based Evolutionary Computation Agent that Learns to Play Mario,
  2010 IEEE Conference on Computational Intelligence and Games (CIG '10) pp. 83-90.


   D. Perez, M. Nicolau, M. O'Neill, and A. Brabazon,
   Evolving Behaviour Trees for the Mario AI Competition Using Grammatical Evolution,
   Proceedings of EvoApps, 2010, pp. 123-132.


  E. R. Speed, 
  Evolving a mario agent using cuckoo search and softmax heuristics,
  Proceedings of the IEEE Consumer Electronics Society's Games Innovations Conference (ICE-GIC), 2010,
  pp. 1-7.
  

  Thomas Willer Sandberg,
  Evolutionary Multi-Agent Potential Field based AI approach for SSC scenarios in RTS games,
   University of Copenhagen,
   2011.
  

  ECJ: A Java-based Evolutionary Computation Research System,
  https://cs.gmu.edu/ eclab/projects/ecj/.
  

  Sean Luke,
  The ECJ Owner's Manual, v23,
  https://cs.gmu.edu/ eclab/projects/ecj/docs/manual/manual.pdf
  George Mason University,
  2015.
  

  Sean Luke,
  ECJ Tutorial 3: Build a Floating-Point Evolution Strategies Problem,
  https://cs.gmu.edu/ eclab/projects/ecj/docs/tutorials/tutorial3/index.html
  George Mason University.
  

  JGAP: Java Genetic Algorithms Package,
  http://jgap.sourceforge.net/.


  Robin Baumgarten,
  A* Mario Agent,
  https://github.com/jumoel/mario-astar-robinbaumgarten.
  

  Ryan Small,
  Agent Smith: a Real-Time Game-Playing Agent for Interactive Dynamic Games,
  GECCO `08, July 12-16, 2008, Atlanta, Georgia, USA.
  http://www.cs.bham.ac.uk/ wbl/biblio/gecco2008/docs/p1839.pdf.
  

  Hartmut Pohlheim,
  GEATbx: Genetic and Evolutionary Algorithm Toolbox for use with MATLAB Documentation,
  version 3.80, 2006,
  http://www.geatbx.com/docu/index.html.
  
 
  Fred Glover and Kenneth Srensen,
  Metaheuristics,
  Scholarpedia, 10(4):6532.,
  http://www.scholarpedia.org/article/Metaheuristics,
  2015.




























